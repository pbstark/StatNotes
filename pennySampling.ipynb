{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penny sampling and Conditional Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penny Sampling\n",
    "\n",
    "\"Penny sampling\" is a sampling and estimation approach closely related to [dollar unit sampling](./dus.ipynb), \n",
    "but that allows particularly simple analysis. It was introduced by Edwards et al. (2013).  \n",
    "\n",
    "We shall derive a continuous version of Penny Sampling that turns the problem of making confidence bounds for the mean of a bounded random variable into the problem of making confidence bounds for a Binomial $p$, by introducing auxiliary independent randomization. \n",
    "\n",
    "\n",
    "### Basic Penny Sampling \n",
    "We shall derive basic Penny Sampling in the context of financial auditing.\n",
    "\n",
    "Again, we have a population of $N$ items; item $j$ has unknown value $x_j$, but the value is known to be nonnegative and less than $u_j$.\n",
    "\n",
    "We divide the upper bound on into \"pennies.\"  Item $j$ contains $u_j$ pennies, of which $x_j$ are \"good\" and $(u_j - x_j)$ are \"bad.\"\n",
    "\n",
    "Let $u := \\sum_{j=1}^N u_j$ and $x := \\sum_{j=1}^N x_j$.\n",
    "\n",
    "We want to estimate the population fraction of \"good\" pennies: \n",
    "\n",
    "\\begin{equation*} \\mu := \\frac{x}{u}.\\end{equation*}\n",
    "\n",
    "We then sample \"pennies\" at random, without replacement, from the entire population.\n",
    "\n",
    "If we draw a penny associated with item $j$, we inspect that item to determine the value $x_j$.\n",
    "\n",
    "If the index of the penny within item $j$ is less than or equal to $x_j$, we consider the penny\n",
    "that was drawn to be \"good.\" Otherwise the penny is \"bad.\"\n",
    "\n",
    "The number of \"good\" pennies in the sample has a hypergeometric distribution with parameters \n",
    "$u$, $x = u \\mu$, and $n$; the expected fraction of \"good\" pennies in the sample is $\\mu = x/u$.\n",
    "\n",
    "We invert hypergeometric tests to find confidence bounds for $x$, which we can translate into confidence bounds\n",
    "for $\\mu$ by dividing by $u$.\n",
    "\n",
    "Again, we are imagining situations in which the sample is very small compared to the number of items in the population&mdash;much less the number of pennies in those items&mdash;so we will act as if we are sampling with replacement, which gives a conservative result in any case.\n",
    "\n",
    "Thus we treat the distribution of the number of \"good\" pennies in the sample as Binomial with parameters $n$ and $p = \\mu = x/u$, rather than hypergeometric.\n",
    "\n",
    "### Relation between penny sampling and PPS/DUS sampling\n",
    "\n",
    "While the chance of selecting each \"penny\" is equal, the chance of selecting item $j$ is proportional to $u_j$, just as in dollar-unit sampling.  The difference is in how the data are analyzed: using $X_i$, the value of $x_j$ for the item selected on the $i$th draw, or using information about whether just the single \"penny\" is good or bad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Penny Sampling and the Conditional Expectation\n",
    "\n",
    "### The Two-Step\n",
    "One can think of penny sampling as taking place in two steps: first, select an item $J$ with PPS sampling, then select a penny uniformly at random from that item. \n",
    "If the penny's index is less than $X_J$, the penny is \"good.\" \n",
    "In other words, pennies are selected conditionally uniformly, given the item they are selected from, \n",
    "and compared to the true value of the item. \n",
    "\n",
    "In the limit as pennies shrink in value, this amounts to comparing $X_J$ to a random variable $Z_J$ that is (continuously) conditionally uniformly distributed on $[0, U_J]$, where $U_J$ is the upper bound on the item selected. Let $W$ be the event that $Z_J \\le X_J$. Then\n",
    "\\begin{align*} \n",
    "    \\mathbb{P} \\{ W = 1 \\} & = \\mathbb{P} \\{ Z_J \\le X_J \\} \\\\\n",
    "    &= \\sum_{j=1}^N \\mathbb P\\{Z_J \\le X_J | J = j\\} \\mathbb{P} \\{J=j\\} \\\\\n",
    "    &= \\sum_{j=1}^N u_j/u \\mathbb P\\{Z_j \\le x_j | J = j\\} \\\\\n",
    "    &= \\sum_{j=1}^N u_j/u \\cdot x_j/u_j \\\\\n",
    "    &= x/u.\n",
    "\\end{align*}\n",
    "In $n$ iid draws, the distribution of the sum of the corresponding values of $W$ is Binomial with parameters $n$ and $p=\\mu = x/u$.\n",
    "\n",
    "Alternatively, we can think of the process as creating a new set of $n$ random variables $\\{W_i\\}$ where $W_i \\sim \\mbox{Bernoulli}(X_i/U_i)$.\n",
    "It is clear that unconditionally, $\\{W_i\\}$ are iid Benoulli($x/u$), so $\\sum_{i=1}^n W_i \\sim \\mbox{Binomial}(n, \\mu)$, where $\\mu := x/u$.\n",
    "\n",
    "### Sequences of iid variables \n",
    "\n",
    "This kind of \"continuous penny sampling\" lets us make confidence bounds for iid samples from an arbitrary bounded distribution, such as the mixture of a uniform and a point-mass at 0. The lower and upper bounds on each draw are 0 and 1. For each $X_i$ in the sample, we generate a uniformly distributed $Z_i$, with all $\\{X_i\\}$ and $\\{Z_i\\}$ independent. Let $W_i = 1_{Z_i \\le X_i}$. \n",
    "Then $\\{W_i \\}_{i=1}^n$ are iid Bernoulli random variables with $\\mathbb{P} \\{W_i = 1\\} = \\mathbb E X_i = \\mu$:\n",
    "\\begin{align*}\n",
    "       \\mathbb{P} \\{ W_i = 1 \\} & = \\mathbb{P} \\{ Z_i \\le X_i \\} \\\\\n",
    "       &= \\int_0^1 \\mathbb{P} \\{ Z_i \\le X_i | Z_i = z\\} dz \\\\\n",
    "       &= \\int_0^1 \\mathbb{P} \\{ X_i > z \\} dz \\\\\n",
    "       &= \\mathbb E X_i,\n",
    "\\end{align*}\n",
    "by the tail-integral formula for expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Let's implement continuous penny sampling in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first cell with code: set up the Python environment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from scipy.stats import binom\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binoLowerCL(n, x, cl = 0.975, inc=0.000001, p = None):\n",
    "    \"Lower confidence level cl confidence interval for Binomial p, for x successes in n trials\"\n",
    "    if p is None:\n",
    "            p = float(x)/float(n)\n",
    "    lo = 0.0\n",
    "    if (x > 0):\n",
    "            f = lambda q: cl - scipy.stats.binom.cdf(x-1, n, q)\n",
    "            lo = sp.optimize.brentq(f, 0.0, p, xtol=inc)\n",
    "    return lo\n",
    "\n",
    "def binoUpperCL(n, x, cl = 0.975, inc=0.000001, p = None):\n",
    "    \"Upper confidence level cl confidence interval for Binomial p, for x successes in n trials\"\n",
    "    if p is None:\n",
    "            p = float(x)/float(n)\n",
    "    hi = 1.0\n",
    "    if (x < n):\n",
    "            f = lambda q: scipy.stats.binom.cdf(x, n, q) - (1-cl)\n",
    "            hi = sp.optimize.brentq(f, p, 1.0, xtol=inc) \n",
    "    return hi\n",
    "\n",
    "def pennySampleReplacement(weights, n):\n",
    "    '''\n",
    "       Weighted random sample of size n drawn with replacement.\n",
    "       Returns indices of the selected items, the \"remainder pennies\" (the conditionally uniform\n",
    "       auxiliary randomization within items) and the raw uniform values used to select the sample\n",
    "    '''\n",
    "    if any(weights < 0):\n",
    "        print('negative weight in weightedRandomSample')\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        totWt = np.sum(weights, dtype=float)\n",
    "        wc = np.cumsum(weights, dtype=float)/totWt  # ensure weights sum to 1\n",
    "        theSam = np.random.random_sample((n,))\n",
    "        inx = np.array(wc).searchsorted(theSam)\n",
    "        penny = [(wc[inx[i]]-theSam[i])*totWt for i in range(n)]\n",
    "        return inx, penny, theSam\n",
    "\n",
    "def pennyBinomialLowerBound(x, inx, pennies, cl=0.95):\n",
    "    '''\n",
    "       Penny sampling lower (one-sided) 1-alpha confidence bound on the mean, for sampling with replacement.\n",
    "       x is the vector of observed values\n",
    "       pennies is the vector of _which_ \"penny\" in each sampled item is to be adjudicated as \"good\" or \"bad\"\n",
    "       The first x_j pennies in item j are deemed \"good,\" the remaining (u_j - x_j) are \"bad.\"\n",
    "       Returns the lower bound and the number of \"good\" pennies in the sample.\n",
    "    '''\n",
    "    s = sum([pennies[i] <= x[inx[i]] for i in range(len(pennies))])\n",
    "    n = len(inx)\n",
    "    return binoLowerCL(n, s, cl=cl), s\n",
    "\n",
    "def pennyBinomialBounds(x, inx, pennies, cl=0.95):\n",
    "    '''\n",
    "       Penny sampling 2-sided confidence interval for the mean, for sampling with replacement.\n",
    "       x is the vector of observed values\n",
    "       pennies is the vector of _which_ \"penny\" in each sampled item is to be adjudicated as \"good\" or \"bad\"\n",
    "       The first x_j pennies in item j are deemed \"good,\" the remaining (u_j - x_j) are \"bad.\"\n",
    "       Returns the lower bound, the upper bound and the number of \"good\" pennies in the sample.\n",
    "    '''\n",
    "    s = sum([pennies[i] <= x[inx[i]] for i in range(len(pennies))])\n",
    "    n = len(inx)\n",
    "    return binoLowerCL(n, s, cl=1-(1-cl)/2), binoUpperCL(n, s, cl=1-(1-cl)/2), s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "Let's imagine a population of $N$ items bounded between 0 and 1.\n",
    "\n",
    "Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability of Student-t and Continuous Penny Sampling confidence intervals for mixture of U[0,1] and pointmass at 0 population</h3><strong>Nominal coverage probability 95.0%</strong>.<br /><strong>Estimated from 10000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 0</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Student-t cov</th>\n",
       "      <th>Penny cov</th>\n",
       "      <th>Student-t len</th>\n",
       "      <th>Penny len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25</td>\n",
       "      <td>89.84%</td>\n",
       "      <td>99.28%</td>\n",
       "      <td>0.6397</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50</td>\n",
       "      <td>98.87%</td>\n",
       "      <td>98.78%</td>\n",
       "      <td>0.6762</td>\n",
       "      <td>0.1382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100</td>\n",
       "      <td>99.97%</td>\n",
       "      <td>98.23%</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.0943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>96.24%</td>\n",
       "      <td>0.6881</td>\n",
       "      <td>0.0452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25</td>\n",
       "      <td>22.18%</td>\n",
       "      <td>99.35%</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50</td>\n",
       "      <td>38.05%</td>\n",
       "      <td>99.86%</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>0.0791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100</td>\n",
       "      <td>62.33%</td>\n",
       "      <td>98.31%</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>0.0448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400</td>\n",
       "      <td>97.68%</td>\n",
       "      <td>98.18%</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25</td>\n",
       "      <td>2.17%</td>\n",
       "      <td>98.8%</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>97.66%</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>99.89%</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400</td>\n",
       "      <td>32.33%</td>\n",
       "      <td>98.11%</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 0  sample size Student-t cov Penny cov Student-t len Penny len\n",
       "0       0.900           25        89.84%    99.28%        0.6397     0.207\n",
       "1       0.900           50        98.87%    98.78%        0.6762    0.1382\n",
       "2       0.900          100        99.97%    98.23%         0.682    0.0943\n",
       "3       0.900          400        100.0%    96.24%        0.6881    0.0452\n",
       "4       0.990           25        22.18%    99.35%        0.0986    0.1455\n",
       "5       0.990           50        38.05%    99.86%        0.1212    0.0791\n",
       "6       0.990          100        62.33%    98.31%        0.1611    0.0448\n",
       "7       0.990          400        97.68%    98.18%        0.2103    0.0168\n",
       "8       0.999           25         2.17%     98.8%        0.0096     0.138\n",
       "9       0.999           50         4.81%    97.66%        0.0141    0.0719\n",
       "10      0.999          100         8.82%    99.89%        0.0179     0.037\n",
       "11      0.999          400        32.33%    98.11%        0.0357    0.0101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at zero and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "\n",
    "simTable = pd.DataFrame(columns=('mass at 0', 'sample size', 'Student-t cov',\\\n",
    "                                 'Penny cov', 'Student-t len', 'Penny len')\n",
    "                       )\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5  #  p*0 + (1-p)*.5\n",
    "    for n in ns:\n",
    "        tCrit = sp.stats.t.ppf(q=1-alpha/2, df=n-1)\n",
    "        coverT = 0\n",
    "        coverP = 0\n",
    "        totT = 0.0\n",
    "        totP = 0.0\n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)\n",
    "            ptMass = np.random.uniform(size=n)\n",
    "            pennies = np.random.uniform(size=n)  # auxiliary uniform randomization within items\n",
    "            sam[ptMass < p] = 0.0                # point mass at zero\n",
    "            samMean = np.mean(sam)\n",
    "            samSD = np.std(sam, ddof=1)\n",
    "            coverT += ( math.fabs(samMean-popMean) < tCrit*samSD )\n",
    "            totT += 2*tCrit*samSD   \n",
    "            pLo, pHi, s = pennyBinomialBounds(sam, np.r_[0:n], pennies, cl=1-alpha )\n",
    "            coverP += ( pLo <= popMean ) & (popMean <= pHi)\n",
    "            totP += pHi - pLo\n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(coverT)/float(reps)) + '%',\\\n",
    "            str(100*float(coverP)/float(reps)) + '%',\\\n",
    "            str(round(totT/float(reps),4)),\\\n",
    "            str(round(totP/float(reps), 4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability of Student-t and Continuous Penny Sampling confidence intervals for ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 0 population</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>.<br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is the case with the other nonparametric methods, the Continuous Penny Sample intervals are in some cases shorter on average than Student-t intervals, but have higher coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
