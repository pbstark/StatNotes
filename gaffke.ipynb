{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6955a67-f8ae-4b1d-b6a3-4d79896c1808",
   "metadata": {},
   "source": [
    "# The Gaffke Conjecture\n",
    "\n",
    "Norbert Gaffke https://www.math.uni-magdeburg.de/institute/imst/ag_gaffke/files/pp1304.pdf and Learned-Miller and Thomas https://arxiv.org/abs/1905.06208 proposed a \n",
    "new test for the mean of a nonnegative random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed52c69-cfd5-46b4-ab26-31e73e5f8ab5",
   "metadata": {},
   "source": [
    "Suppose $X$ is a nonnegative random variable with CDF $F$. \n",
    "\n",
    "The expected value of $X$ is\n",
    "\\begin{equation}\n",
    "\\mathbb{E}X = \\int_0^\\infty xdF(x).\n",
    "\\end{equation}\n",
    "Recall the identity for nonnegative random variables:\n",
    "\\begin{eqnarray} \n",
    "\\int_0^\\infty (1-F(x))dx &=& \\int_0^\\infty \\mathbb{P}(X>x) dx \\\\\n",
    "&=& \\int_0^\\infty dx \\int 1_{y > x} dF(y) \\\\\n",
    "&=& \\int dF(y) \\int_0^\\infty 1_{y > x} dx \\mbox{ (by Tonelli's theorem)}\\\\\n",
    "&=&  \\int dF(y) y \\\\\n",
    "&=& \\mathbb{E}X\n",
    "\\end{eqnarray}\n",
    "\n",
    "Define $F^{-1}(p) := \\inf \\{x: F(x) \\ge p \\}$, the generalized inverse of $F$.\n",
    "Then $F(F^{-1}(p)) \\ge p$ for all $p \\in [0, 1]$.\n",
    "Since $F$ is monotone increasing, if $U \\sim U[0, 1]$,\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{F^{-1}(U) \\le x \\} = \\mathbb{P} \\{F(F^{-1}(U)) \\le F(x) \\} = \\mathbb{P} \\{U \\le F(x) \\} = F(x).\n",
    "\\end{equation}\n",
    "Thus $F^{-1}(U) \\sim F$. \n",
    "(Note: this derivation isn't quite rigorous, but the result is true.)\n",
    "This identity is behind a common way of generating IID observations from an arbitrary distribution:\n",
    "generate IID uniform random variables and apply $F^{-1}$ to them.\n",
    "\n",
    "Now if $X \\sim F$, \n",
    "\\begin{equation}\n",
    "\\mathbb{E} X = \\mathbb{E}(F^{-1}(U)) = \\int_0^1 F^{-1}(p) dp.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec4f80-11b5-426f-aee0-d16a90bad274",
   "metadata": {},
   "source": [
    "## Motivation/Intuition for Gaffke's conjecture\n",
    "\n",
    "Let $\\{U_j\\}_{j=1}^n$ be IID $U[0, 1]$, so $\\{F^{-1}(U_j)\\}_{j=1}^n$ are IID $F$.\n",
    "If we have a sample $\\{X_j\\}$ IID $F$, we can think of them as $\\{F^{-1}(U_j)\\}_{j=1}^n$.\n",
    "Let $(u_{(j)})_{j=1}^n$ be increasing values in $[0, 1]$ and let $u_{(n+1)} := 1$. \n",
    "\\begin{equation}\n",
    "\\mathbb{E}X = \\int_0^1 F^{-1}(p) dp \\ge \\sum_{j=1}^n F^{-1}(u_{(j)})(u_{(j+1)} - u_{(j)}),\n",
    "\\end{equation}\n",
    "since $F^{-1}$ is an increasing function (the Riemann sum using the left values is less than the integral).\n",
    "So we could lower-bound the mean by \n",
    "\\begin{equation}\n",
    "\\sum_{j=1}^n X_{(j)}(U_{(j+1)} - U_{(j)})\n",
    "\\end{equation}\n",
    "if we could observe $\\{U_j\\}$, but we can't. \n",
    "However, we could imagine simulating IID uniform random variables and using the observed\n",
    "values of $X_j$, and accounting for the simulation uncertainty by using a small quantile of\n",
    "the resulting distribution.\n",
    "That is essentially Gaffke's method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796803b-26f6-4eb1-97d2-88485ae0429b",
   "metadata": {},
   "source": [
    "Suppose $\\{X_j\\}_{j=1}^n$ are IID nonnegative random variables with mean $\\mu$.\n",
    "Let  $\\{U_j\\}_{j=1}^n$ be IID $U[0, 1]$ independent of $\\{X_j\\}$.\n",
    "Let $U_{(j)}$ be the $j$th order statistic of $\\{U_j\\}$, with $U_{(n+1)} := 1$.\n",
    "Let $x = (x_j)_{j=1}^n$.\n",
    "Consider the function\n",
    "\\begin{equation}\n",
    "K(x) := \\mathbb{P}_U \\left ( \\sum_{j=1}^n x_{(j)} (U_{(j+1)} - U_{(j)})  \\le \\mu \\right ) \n",
    "\\end{equation}\n",
    "\n",
    "Gaffke conjectures that\n",
    "\\begin{equation}\n",
    "\\mathbb{P}\\{ K(X) \\le \\alpha \\} \\le \\alpha.\n",
    "\\end{equation}\n",
    "(Indeed, he conjectures that it holds if $\\{X_j\\}$ are independent, nonnegative, and have the same mean $\\mu$, even\n",
    "if they are not identically distributed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a13049-47b9-4ff3-b73c-663fd0a1ccc7",
   "metadata": {},
   "source": [
    "Extensive simulations have not found a case where it fails, but the general case has not been\n",
    "proved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b5fb-d819-4181-a301-7cc573a45477",
   "metadata": {},
   "source": [
    "# Proof of some special cases\n",
    "\n",
    "For $n=1$, the test is equivalent to Markov's inequality:\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{P}_X \\left \\{ \\mathbb{P}_U \\sum_{j=1}^n X_{(j)} (U_{(j+1)} - U_{(j)}) \\le \\mu \\} \\le \\alpha \\right \\} &=& \\mathbb{P}_X \\{ \\mathbb{P}_U X(1-U) \\le \\mu \\} \\le \\alpha \\} \\\\\n",
    "   &=& \\mathbb{P}_X \\{ \\mathbb{P}_U XU \\le \\mu \\} \\le \\alpha \\} \\\\\n",
    "   &=& \\mathbb{P}_X \\{ \\mathbb{P}_U U \\le \\mu/X \\} \\le \\alpha \\} \\mbox{ (this assumes $X \\ne 0$, but it is true regardless)}\\\\\n",
    "   &=& \\mathbb{P}_X \\{ \\mu/X \\wedge 1 \\le \\alpha \\} \\} \\\\\n",
    "   &=& \\mathbb{P}_X \\{ \\mu/X \\le \\alpha \\} \\} \\\\ \n",
    "   &=& \\mathbb{P}_X \\{ X \\ge \\mu/\\alpha \\} \\} \\\\ \n",
    "   &\\le& \\alpha.  \\mbox{ (by Markov's inequality)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcaa94-170d-4cbd-af01-d41fe4cb10b3",
   "metadata": {},
   "source": [
    "For arbitrary $n$, if the population is binary, the test is equivalent to the standard, most powerful\n",
    "Binomial test, and if the support of $X_j$ contains only two (nonnegative) points, the test remains valid, \n",
    "as we shall see.\n",
    "\n",
    "Suppose $X_i \\sim \\mathrm{Bern}(p)$, so \n",
    "\\begin{equation}\n",
    "F(x) := \\left \\{ \\begin{array}{ll}\n",
    "     1-p, & x \\in [0,1) \\\\\n",
    "     1, &  x \\ge 1,\n",
    "     \\end{array}\n",
    "     \\right .\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "F^{-1}(q) = \n",
    "\\begin{cases} \n",
    "0, & q \\in [0, 1-p) \\\\\n",
    "1, &  q \\in [1-p, 1].\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "Let $S := \\sum_j X_j \\sim \\mathrm{Binom}(n,p)$ be the sample sum.\n",
    "Let $\\mathbf{U} = \\{U_j\\}_{j=1}^n$ be IID $U[0,1]$, independent of $\\{X_j\\}$, and let $U_{(n+1)} := 1$.\n",
    "\\begin{align}\n",
    "    \\sum_{i=1}^n X_{(i)} \\left (U_{(i+1)} - U_{(i)} \\right ) \n",
    "    &= \n",
    "    \\sum_{n-S+1}^n \\left (U_{(i+1)} - U_{(i)} \\right ) \\nonumber \\\\\n",
    "    &= \n",
    "    1 - U_{(n-S+1)}.\n",
    "\\end{align}\n",
    "By symmetry, $1-U_{(n-S+1)}$ has the same distribution as\n",
    "$U_{(S)}$.\n",
    "We are thus interested in \n",
    "$\\mathbb{P} \\{U_{(S)} \\le p\\} = \\mathbb{E} 1_{U_{(S)} \\le p}$.\n",
    "\n",
    "The event $U_{(s)} \\le p$ is the event that there are $s$ or more \"successes\" in $n$ independent $\\mathrm{Bernoulli}(p)$ trials, i.e., the upper tail probability of a $\\mathrm{Binom}(n,p)$ random variable from $s$ to $n$.\n",
    "Conditional on $X$ (and thus on $S=s$), \n",
    "\\begin{equation}\n",
    " \\mathbb{P}_{\\mathbf{U}} \\{U_{(s)} \\le p \\} =\n",
    "  \\sum_{j=s}^n \\binom{n}{j}p^j(1-p)^{n-j}. \n",
    "\\end{equation}\n",
    "We need \n",
    "\\begin{equation}\n",
    "\\mathbb{P}_S \\left \\{ \\sum_{j=S}^{n} \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha \\right \\}.\n",
    "\\end{equation}\n",
    "Partitioning on $S$ gives\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{P}_S \\left \\{ \\sum_{j=S}^n \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha \\right \\} &=&\n",
    "\\sum_{s=0}^n \\mathbf{1}_{\\sum_{j=s}^n \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha} \\mathbb{P}_S(S=s) \\nonumber \\\\\n",
    "&=& \n",
    "\\sum_{s=0}^n \\mathbf{1}_{\\sum_{j=s}^n \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha}\n",
    "\\binom{n}{s} p^s (1-p)^{n-s}.\n",
    "\\end{eqnarray}\n",
    "The indicator is zero when $s$ is small, and becomes $1$ once $s$ is sufficiently large.\n",
    "The smallest $s$ for which the indicator is $1$ is the\n",
    "first $s$ for which the upper tail probability is not greater than $\\alpha$.\n",
    "I.e.,\n",
    "\\begin{eqnarray}\n",
    "\\sum_{s=0}^n \\mathbf{1}_{\\sum_{j=0}^s \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha}\n",
    "\\binom{n}{s} p^s (1-p)^{n-s} &=&\n",
    "\\sum_{s: \\sum_{j=s}^n \\binom{n}{j}p^j(1-p)^{n-j} \\le \\alpha} \\binom{n}{s} p^s (1-p)^{n-s} \\nonumber \\\\\n",
    "&\\le& \\alpha. \\;\\;\\Box\n",
    "\\end{eqnarray}\n",
    "This is thus equivalent to the most powerful one-sided test for a Binomial $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac221d-9c94-4764-8c5e-1eb0b0b6197b",
   "metadata": {},
   "source": [
    "## General 2-point distributions\n",
    "\n",
    "We now consider IID nonnegative random variables \n",
    "$\\{X_i\\}$ with\n",
    "two points of support, $\\{a, b\\}$, $0 \\le a < b$, with mass $(1-p)$ at $a$ and mass $p$ at $b$.\n",
    "Then $\\mathbb{E} X_i := \\mu = (1-p)a + pb$.\n",
    "If we knew $a$ and $b$, we could transform to $Y_i := \\frac{X_i-a}{b-a}$. \n",
    "These $Y_i$ are\n",
    "IID $\\mathrm{Bernoulli}(p)$, for which we just saw Gaffke's conjecture holds.\n",
    "\n",
    "We do not actually need to know $a$ and $b$ and transform $X_i$ that way in order to obtain a valid \n",
    "P-value:\n",
    "\\begin{eqnarray}\n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n X_{(i)} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\mu \\right \\}\n",
    "& = &\n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\frac{\\sum_{i=1}^n X_{(i)} \\left (U_{(i+1)} - U_{(i)} \\right ) - a}{b-a} \\le \\frac{\\mu-a}{b-a} \\right \\} \\nonumber \\\\\n",
    " & = & \n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \n",
    " \\frac{X_{(i)}-a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) + \\right . \\nonumber \\\\\n",
    " && \\left . + \\; \\frac{a}{b-a} \\left [ \\sum_{i=1}^n \\left (U_{(i+1)} - U_{(i)} \\right ) -1 \\right ] \\le \\frac{\\mu-a}{b-a} \\right \\} \\nonumber \\\\\n",
    " &=& \n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \n",
    " \\frac{X_{(i)}-a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) + \\right . \\nonumber \\\\\n",
    " && \\left . + \\; \\frac{a}{b-a} \\left [ 1 - U_{(1)} - 1 \\right ] \\le \\frac{\\mu-a}{b-a} \\right \\} \\nonumber \\\\\n",
    " &=&\n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \\frac{X_{(i)} -a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\frac{\\mu-a(1-U_{(1)})}{b-a} \\right \\} \\nonumber \\\\\n",
    " &\\ge& \n",
    " \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \\frac{X_{(i)} -a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\frac{\\mu-a}{b-a} \\right \\}.\n",
    " \\nonumber \\\\\n",
    " && {}\n",
    "\\end{eqnarray}\n",
    "The final inequality implies that Gaffke with $X_i$ is only less than $\\alpha$ when Gaffke with (Bernoulli) $Y_i$ is less than $\\alpha$. \n",
    "\n",
    "In symbols,\n",
    "\\begin{equation}\n",
    "    \\left ( \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n X_{(i)} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\mu \\right \\} \\le \\alpha \\right )\n",
    "    \\subset\n",
    "    \\left (\\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \\frac{X_{(i)} -a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\frac{\\mu-a}{b-a} \\right \\} \\le \\alpha \\right ).\n",
    "\\end{equation}\n",
    "Then by the Gaffke result for Bernoulli random variables, we obtain:\n",
    "\\begin{eqnarray}\n",
    " \\mathbb{P}_\\mathbf{X} \\left ( \\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n X_{(i)} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\mu \\right \\} \\le \\alpha \\right )\n",
    "& \\le &\n",
    "   \\mathbb{P}_\\mathbf{X} \\left (\\mathbb{P}_\\mathbf{U} \\left \\{ \\sum_{i=1}^n \\frac{X_{(i)} -a}{b-a} \\left (U_{(i+1)} - U_{(i)} \\right ) \\le \\frac{\\mu-a}{b-a} \\right \\} \\le \\alpha \\right ) \\nonumber \\\\\n",
    "   &\\le& \\alpha\n",
    "\\end{eqnarray}\n",
    "Gaffke is thus conservative if the lower bound of the two-point distribution is greater than 0, even if we do not know what this lower bound is. Since the last inequality in (11) is strict if $a > 0$, Gaffke is less sharp if we do not use the correct lower bound. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf6bb9-6fb3-4d36-808f-df3588e711b8",
   "metadata": {},
   "source": [
    "## Open problem\n",
    "\n",
    "A number of smart people have spent time trying to prove Gaffke's result. \n",
    "No proof is known, to the best of my knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
