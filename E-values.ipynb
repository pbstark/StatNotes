{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b6a7c0-ef74-4a84-8065-efe9176d86ca",
   "metadata": {},
   "source": [
    "# $E$-values and Betting Scores\n",
    "\n",
    "Core references: \n",
    "\n",
    "+ Grünwald, P., R. de Heide, and W. Koolen, 2023. Safe Testing. https://arxiv.org/abs/1906.07801\n",
    "\n",
    "+ Shafer, G., 2021. Testing by betting: A strategy for statistical and scientific communication,\n",
    "_Journal of the Royal Statistical Society Series A: Statistics in Society_, _184_, 407–431, https://doi.org/10.1111/rssa.12647\n",
    "\n",
    "+ Vovk, V., and R. Wang, 2021. E-values: Calibration, combination and applications, _Ann. Statist. 49_ (3) 1736-1754. https://doi.org/10.1214/20-AOS2020\n",
    "\n",
    "+ Lecture notes by V. Vovk. https://www.isibang.ac.in/~statmath/pcm2020/talk1.pdf, https://www.isibang.ac.in/~statmath/pcm2020/talk2.pdf\n",
    "\n",
    "+ Wang, R., and A. Ramdas, 2021. False discovery rate control with e-values, https://arxiv.org/pdf/2009.02824.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7737414-543b-401e-b567-c2fa0f99efc4",
   "metadata": {},
   "source": [
    "$E$-values are a way of quantifying evidence about a statistical hypothesis. \n",
    "They are closely related to $P$-values, but more general in many ways, and possibly easier to understand.\n",
    "In particular (paraphrasing Shafer, 2021):\n",
    "\n",
    "+ An $E$-value is the observed value of a nonnegative random variable whose expected value under the null is 1: $\\mathbb{E}_0 E = 1$. In contrast, a $P$-value is the observed value of a nonnegative random variable whose probability distribution under the null is dominated by the uniform distribution: $\\mathbb{P}_0 \\{P \\le x\\} \\le x$, $\\forall x \\in [0, 1]$. It is generally a much more straightforward to construct $E$-values than $P$-values.\n",
    "\n",
    "+ $E$-values are like the returns on a bet. Most people know it's possible to win a lot of money by \"getting lucky\"\n",
    "and winning a bet with long odds, or by identifying bets where the payoff odds don't reflect the chance odds. Fewer people understand $P$-values. It's common to think that a small $P$-value means the alternative is true or that the probability that the null is true is small--two common misconceptions.\n",
    "\n",
    "+ Any particular bet implies an alternative hypothesis. The betting score is the likelihood of the alternative divided by the likelihood of the null. Likelihood ratios have intuitive appeal.\n",
    "\n",
    "+ Power calculations involve a fixed significance level, not a $P$-value, so there's no direct analog of power for $P$-values. In contrast, a bet also implies a target: a value for the betting score that might be expected if the alternative hypothesis is true. \n",
    "\n",
    "+ The validity of $P$-values generally requires pre-specifying the entire analysis, but betting scores can include arbitrarily complex strategies to \"win\" that use all currently available data to inform the next bet.  Betting scores thus may correspond better to how Science is conducted: a single hypothesis might be tested many times, and each experiment (including its design, what is measured, and the test used) might be informed by previous experiments--and the alternative may evolve from new information in other fields.\n",
    "\n",
    "+ Betting scores can often be combined by multiplication, which corresponds to \"reinvesting\" the winnings in future bets, and can always be combined using averages (with or without weights). In contrast, combining $P$-values is much more subtle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7476acf-d419-4af1-9615-8d738ec87a68",
   "metadata": {},
   "source": [
    "## Notation\n",
    "In this chapter, we will use the notation $\\mathbb{P}(\\cdot) := \\mathbb{E}_P (\\cdot)$ to denote expectation with respect to the distribution $P$.\n",
    "The expected value of the indicator function of a measurable set $A$ is the \n",
    "probability of $A$; generalizing from $\\mathbb{E}_P (1_A) = \\mathbb{P}(A)$ to the expectation of other functions.\n",
    "\n",
    "Any distributions that appear in the same expression will be assumed to have a single dominating measure\n",
    "$\\mu$, so we can talk about densities with respect to $\\mu$.\n",
    "The density of $\\mathbb{P}$ with respect to $\\mu$ will be denoted $f_\\mathbb{P}$, so that \n",
    "$d\\mathbb{P}(\\omega) = f_\\mathbb{P}(\\omega) d\\mu(\\omega)$.\n",
    "\n",
    "Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space.\n",
    "Let $\\mathcal{I}$ be a totally ordered set with order relation $\\le$.\n",
    "Suppose that for all $i \\in \\mathcal{I}$, $\\mathcal{F}_i$ is a sub-sigma-algebra of $\\mathcal{A}$,\n",
    "and that if $i < j$, $\\mathcal{F}_i \\subset \\mathcal{F}_j$.\n",
    "Then $\\mathbb{F} := \\{\\mathcal{F}_i\\}_{i \\in \\mathcal{I}}$ is a _filtration_ and $(\\Omega, \\mathcal{A}, \\mathbb{F}, \\mathbb{P})$ is\n",
    "a _filtered probability space_.\n",
    "\n",
    "Filtrations arise naturally in studying stochastic processes.\n",
    "Let $\\sigma(X)$ be the sigma-algebra generated by the random variable $X$ (the smallest sigma-algebra for which $X$ is measurable, i.e., the smallest sigma algebra that contains the pre-image $X^{-1}(B)$\n",
    "of every Borel subset $B \\subset \\mathcal{B}$), and let  $\\sigma(X_j : j \\le i) := \\sigma(\\cup_{j \\le i} \\sigma(X_j))$.\n",
    "As the process evolves, a richer and richer set of events becomes measurable.\n",
    "Let $(X_i)_{i \\in \\mathbb{N}}$ be a stochastic process on the probability space $(\\Omega, \\mathcal{A}, \\mathbb{P})$,\n",
    "and define $\\mathcal{F}_i := \\sigma(X_j : j \\le i)$.\n",
    "Then $\\mathbb{F} := \\{\\mathcal{F}_i\\}_{i \\in \\mathcal{I}}$ is a filtration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c17708-21e3-41f1-9048-c3fde20d6620",
   "metadata": {},
   "source": [
    "## Warm-up 1: betting as evidence\n",
    "\n",
    "A _predictor_ or _forecaster_ claims that $Y$ is a random variable with probability distribution $\\mathbb{P}_0$.\n",
    "The value $y$ of $Y$ will be revealed (to the predictor and you) later.\n",
    "The predictor backs up the claim by offering to sell you any payoff $S(y)$ for the price $\\mathbb{E}_0 S(Y) =: \\mathbb{P}_0 S(Y)$,\n",
    "the expected value of $S(Y)$ (before it is observed), computed\n",
    "on the assumption that the predictor is right--that $Y$ is indeed a random variable and has distribution $\\mathbb{P}_0$.\n",
    "The payoff is required to be nonnegative, so that all you risk is what you bet: the expected value of $S$ on the assumption that $Y \\sim \\mathbb{P}_0$.\n",
    "\n",
    "If you buy the bet $S$, your payoff is $S(y)$ if $Y=y$, and your _betting score_ is $S(y)/\\mathbb{P}_0 S(Y)$, the amount by which you multiplied your initial stake.\n",
    "Without loss of generality, we may assume that $\\mathbb{P}_0 S(Y) = 1$ and allow you to buy any multiple of the bet $S$ you can afford; then your (eventual) betting score is $S(y)$. \n",
    "You don't have to bet your whole fortune, but if you withhold a fraction $\\beta$ of your current fortune and \n",
    "bet the remaining fraction $1-\\beta$ on $S$, that is equivalent to betting your whole fortune on $S' = \\beta + (1-\\beta)S$, which also has expected value 1 under the null since $\\mathbb{P}_0 S = 1$.\n",
    "That is, betting only a fraction of your current fortune its just another bet that is expected to break even\n",
    "under the predictor's hypothesis, so without loss of generality, we can assume that you bet your entire fortune\n",
    "on some $S$.\n",
    "\n",
    "It isn't necessary that *you* believe $Y$ is really a random variable: you can still\n",
    "bet if you think the predictor's claim is wrong, that is, if you think you can make money betting on some\n",
    "$S$ with $\\mathbb{P}_0 S(Y) = 1$.\n",
    "\n",
    "Now suppose there is a series of trials, $(Y_j)$, which might or might not be random; and if they are random,\n",
    "they might or might not be independent.\n",
    "The predictor is allowed to make a series of predictions, say $\\mathbb{P}_{0j}$ for\n",
    "$j = 1, \\ldots$.\n",
    "The predictor need not make a prediction for every trial, \n",
    "and the prediction for the $j$th trial, $\\mathbb{P}_{0j}$, might depend on the outcome of previous trials, $\\{Y_i \\}_{i<j}$.\n",
    "(This is much closer to how science is conducted than the assumption that trials are independent\n",
    "and involve the same parameters.)\n",
    "Shafer (2021) writes:\n",
    "\n",
    "> The probabilistic predictions that can be associated with a scientific hypothesis usually go beyond a single comprehensive probability distribution. In some cases, a scientist may begin with a joint probability distribution P for a sequence of variables $Y_1 , \\ldots, Y_N$  and formulate a plan for successive experiments that will allow her to observe them. But the scientific enterprise is usually more opportunistic. A scientist might perform an experiment that produces $Y_1$’s value $y_1$ and then decide whether it is worthwhile to perform the further experiment that would produce $Y_2$’s value $y_2$. Perhaps no one even thought about $Y_2$ at the outset. One scientist or team tests the hypothesis using $Y_1$, and then, perhaps because the result is promising but not conclusive, some other scientist or team comes up with the idea of further testing the hypothesis with a second variable $Y_2$ from a hitherto uncontemplated new experiment or database.\n",
    "\n",
    "\n",
    "Suppose you start with \\\\$1, and you are allowed to bet on any or all of the predictions: before the $j$th \n",
    "trial the predictor offers the prediction $Y_j \\sim \\mathbb{P}_{j0}$, which can depend on previous trials.\n",
    "You are allowed to buy any nonnegative $S(Y_j)$ for the price $\\mathbb{P}_{j0} S(Y_j)$, which is assumed to\n",
    "be \\\\$1. \n",
    "Your fortune after the first bet is settled is $S_1(y_1)$. \n",
    "The predictor now offers to sell you any $S_2(Y_2)$ for its expected value under the null $Y_2 \\sim \\mathbb{P}_{02}$,\n",
    "again assumed to be \\\\$1.\n",
    "If you bet your current fortune to by a multiple of $S_2(Y_2)$, then\n",
    "your fortune when the second bet settles is $S_1(y_1)S_2(y_2)$, etc.: betting scores on successive bets multiply to give your current fortune.\n",
    "\n",
    "If you end up making a lot of money, that is evidence that the predictor was wrong--or that you were very lucky. \n",
    "If you don't end up making a lot of money, maybe the predictor was right--or maybe you chose bad bets (you didn't bet\n",
    "on the right alternative).\n",
    "Regardless, it is not evidence that the predictor was right.\n",
    "This is the same asymmetry involved in hypothesis tests: a large $P$-value is not evidence that the null is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6455d3c-298a-4bdf-b7d0-a1282b14f7ec",
   "metadata": {},
   "source": [
    "## Warm-up 2: hypothesis tests as bets\n",
    "\n",
    "See [hypothesis testing](./tests.ipynb).\n",
    "\n",
    "Core idea: if you can make money betting against the null hypothesis (by making bets\n",
    "that are expected to be break-even of the null hypothesis is true), that's evidence that the\n",
    "null hypothesis is false.\n",
    "\n",
    "In the typical setup for hypothesis testing, we observe data $X \\sim \\mathbb{P}$.\n",
    "To test the null hypothesis test $\\mathbb{P} = \\mathbb{P}_0$, we choose a function $\\phi(\\cdot)$ with the property that $\\mathbb{P}_{\\mathbb{P}_0,U}\\phi(X,U) = \\alpha$, where $U$ is an auxilliary uniform random variable\n",
    "independent of $X$, only needed for randomized tests.\n",
    "\n",
    "We reject the hypothesis $\\mathbb{P} = \\mathbb{P}_0$ if $U \\le \\phi(X)$.\n",
    "\n",
    "We can think of $\\phi(X)$ as an \"all-or-nothing\" bet that pays $1/\\alpha$ times the stake (which we will \n",
    "take to be \\\\$1) if $U \\le \\phi(X)$, and pays 0 otherwise. \n",
    "If the null is true, the expected value of such a bet is \\\\$1.\n",
    "That is, $X$ plays the role of $Y$, above, and $S(Y)$ is $1/\\alpha$ if $U \\le \\phi(Y)$ and zero otherwise.\n",
    "\n",
    "Two scenarios:\n",
    "+ bet once in a while, don't reinvest your winnings\n",
    "+ bet whenever you want, reinvest your winnings\n",
    "\n",
    "The first is like $P$-values and standard tests of significance: all-or-nothing bets, \n",
    "with no \"combining evidence\" across experiments.\n",
    "The second leads to betting scores and $E$-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cd95c-a60b-4c90-bec4-3b317f5b2da4",
   "metadata": {},
   "source": [
    "Multiple testing: suppose a hypothesis is tested 20 times at significance level 5%, producing one\n",
    "\"significant\" result. From a testing perspective, we have to adjust for multiplicity to understand\n",
    "how strong the evidence is that the null is false, and that adjustment requires knowing the dependence among the experiments. From an $E$-value perspective, the betting score is 1: \\\\$20 was wagered, and \\\\$20 was won."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1191e-8466-4567-9512-2ec7f6633d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Betting scores for simple nulls are likelihood ratios, and vice versa\n",
    "\n",
    "Suppose we have a nonnegative random variable $S(Y)$ with expected value $1$ under the null $Y \\sim \\mathbb{P}_0$,\n",
    "i.e., $\\int S(y) d\\mathbb{P}_0(y) = 1$. \n",
    "Thus the measure $\\mathbb{Q}$ defined by $d\\mathbb{Q}(y) := S(y) d\\mathbb{P}_0(y)$ is also a probability measure: $\\mathbb{Q}(y) \\ge 0$ and $\\int d\\mathbb{Q}(y) = 1$.\n",
    "Hence, $S(y) = f_\\mathbb{Q}(y)/f_{\\mathbb{P}_0}(y)$ is the likelihood ratio of $\\mathbb{Q}$ to $\\mathbb{P}_0$.\n",
    "The distribution $\\mathbb{Q}$ is called _the alternative implied by $S$_.\n",
    "\n",
    "Conversely, suppose $\\mathbb{Q}$ is a probability distribution for $Y$.\n",
    "Then $S(Y) := f_\\mathbb{Q}(Y)/f_{\\mathbb{P}_0}(Y)$ is a betting score, since it is nonnegative and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{P}_0 (f_\\mathbb{Q}(Y)/f_{\\mathbb{P}_0}(Y)) &=& \\int (f_\\mathbb{Q}(y)/f_{\\mathbb{P}_0}(y)) d\\mathbb{P}_0(y) \\\\\n",
    "&=& \\int (f_\\mathbb{Q}(y)/f_{\\mathbb{P}_0}(y)) f_{\\mathbb{P}_0}(y) d\\mu(y) \\\\\n",
    "&=& \\int f_\\mathbb{Q}(y) d\\mu(y) \\\\\n",
    "&=& \\int d\\mathbb{Q}(y) \\\\\n",
    "&=& 1.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8911d72-c024-4d8b-a5f6-c6b126a3695f",
   "metadata": {},
   "source": [
    "As mentioned above, the betting formulation makes sense even if $Y$ isn't a random variable.\n",
    "But suppose I think $Y \\sim \\mathbb{Q} \\ne \\mathbb{P}_0$.\n",
    "What payoff function $S$ should I bet on?\n",
    "\n",
    "If the goal is to grow my capital at the fastest rate (the Kelly criterion), I want to maximize\n",
    "\\begin{equation}\n",
    "\\mathbb{Q} \\ln S = \\mathbb{Q} \\ln f_\\mathbb{R}(Y)/f_{\\mathbb{P}_0}(Y)\n",
    "\\end{equation}\n",
    "for some measure $R$.\n",
    "Gibbs' inequality says that\n",
    "\\begin{equation}\n",
    "\\mathbb{Q} f_\\mathbb{Q}(Y)/f_{\\mathbb{P}_0}(Y) \\ge \\mathbb{Q} f_\\mathbb{R}(Y)/f_{\\mathbb{P}_0}(Y)\n",
    "\\end{equation}\n",
    "for any distribution $\\mathbb{R}$ for $Y$ (dominated by $\\mu$).\n",
    "\n",
    "Thus the optimal payoff function to bet on is $S(Y) = f_\\mathbb{Q}(Y)/f_{\\mathbb{P}_0}(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e74e6-5084-4264-a879-7f4a8380c57a",
   "metadata": {},
   "source": [
    "## Why maximize the expected log payoff?\n",
    "\n",
    "This is connected to the idea of repeated betting, rather than one-shot bets.\n",
    "As noted previously, if you maximize the expected return on a single bet, you risk going broke.\n",
    "Maximizing the expected return puts all your money on the single outcome with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8268854-0d73-478d-a785-a7c8bb17f2ca",
   "metadata": {},
   "source": [
    "## Implied targets\n",
    "\n",
    "If you bet on $S$, implicitly you are suggesting that $Y \\sim \\mathbb{Q}$, \n",
    "where $f_\\mathbb{Q}(y) := S(y) f_{\\mathbb{P}_0}(y)$.\n",
    "Implicitly, you expect \n",
    "\\begin{eqnarray}\n",
    "\\mathbb{Q} \\ln S(Y) &=& \\int \\ln S(y) d\\mathbb{Q}(y) \\\\\n",
    "&=&  \\int \\ln S(y) S(y) d\\mathbb{P}_0(y) \\\\\n",
    "&=& \\mathbb{P}_0 S(Y) \\ln S(Y).\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde25bf-131a-4c33-aa13-21afd238b306",
   "metadata": {},
   "source": [
    "## Composite nulls\n",
    "\n",
    "Suppose that the forecaster claims that $Y \\sim \\mathbb{P}$ for some (otherwise unspecified) $\\mathbb{P} \\in \\mathcal{P}_0$, i.e., the null hypothesis is composite, rather than simple.\n",
    "We can test such a hypothesis using betting by using nonnegative payoffs $S(Y)$ that have expected value no greater than 1\n",
    "for any $\\mathbb{P} \\in \\mathcal{P}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000adc9c-dbc8-4893-8a57-6e527da94271",
   "metadata": {},
   "source": [
    "# Betting-based testing protocols for statistical models\n",
    "\n",
    "Suppose a statistical model gives a probability distribution $\\mathbb{P}_0$ for data $Y$\n",
    "\n",
    "The statistician is the _Skeptic_. _Reality_ reveals the value of random variables.\n",
    "\n",
    "**Protocol 1:**\n",
    "\n",
    "+ Skeptic selects a random variable $S \\ge 0$ such that $\\mathbb{P}_0 S(Y) = 1$.  \n",
    "+ Reality announces $y$\n",
    "+ $K := S(y)$.\n",
    "\n",
    "$\\mathbb{P}_0 (K \\ge 1/\\alpha) \\le \\alpha$ for all $\\alpha \\in (0, 1]$, by Markov's inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920d2c6-02e0-40c3-bd7f-feb42a8e1471",
   "metadata": {},
   "source": [
    "## E-values formalized\n",
    "\n",
    "**Definition.**\n",
    "Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space,\n",
    "and let $E$ be a random variable $E: \\Omega \\rightarrow [0, \\infty]$ such that\n",
    "$\\mathbb{P}_\\mathbb{P}(E) := \\int_{\\mathcal{X}} E d\\mathbb{P} \\le 1$. (Note that $E$ may take the value $\\infty$, which\n",
    "corresponds to the strongest possible evidence that the data do not come from $\\mathbb{P}$.)\n",
    "Then **$E$ is an e-variable for $\\mathbb{P}$.**\n",
    "\n",
    "Let $\\mathcal{P}$ be a collection of probability distributions on the measurable space $(\\Omega, \\mathcal{A})$,\n",
    "and let $E$ be a random variable $E: \\Omega \\rightarrow [0, \\infty]$ such that for all $\\mathbb{P} \\in \\mathcal{P}$,\n",
    "$\\mathbb{P}_\\mathbb{P}(E) := \\int_{\\mathcal{X}} E d\\mathbb{P} \\le 1$.\n",
    "Then **$E$ is an e-variable for $\\mathcal{P}$.**\n",
    "\n",
    "The set of all $E$-variables for a collection $\\mathcal{P}$ of probability distributions is $\\mathcal{E}(\\mathcal{P})$.\n",
    "\n",
    "The observed value of an $E$-variable is an $E$-value (or $e$-value).\n",
    "\n",
    "**Definition.**\n",
    "Let $(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space,\n",
    "and let $P$ be a random variable $P: \\Omega \\rightarrow [0, 1]$ such that\n",
    "$\\forall p \\in [0, 1]$,\n",
    "$\\mathbb{P}(P \\le p) \\le p$.\n",
    "Then **$P$ is a P-variable for $\\mathbb{P}$.**\n",
    "\n",
    "Let $\\mathcal{P}$ be a collection of probability distributions on the measurable space $(\\Omega, \\mathcal{A})$,\n",
    "and let $P$ be a random variable $P: \\Omega \\rightarrow [0, 1]$ such that for all $\\mathbb{P} \\in \\mathcal{P}$,\n",
    "$\\forall p \\in [0, 1]$,\n",
    "$\\mathbb{P}(P \\le p) \\le p$.\n",
    "Then **$P$ is a P-variable for $\\mathcal{P}$.**\n",
    "\n",
    "The set of all $P$-variables for a collection $\\mathcal{P}$ of probability distributions is $\\mathcal{P}(\\mathcal{P})$.\n",
    "\n",
    "\n",
    "The observed value of a $P$-variable is a $P$-value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029036d-a146-4b18-ac35-f0e6afd62e28",
   "metadata": {},
   "source": [
    "**$P$ to $E$ calibration function**\n",
    "\n",
    "Suppose $f : [0, 1] \\rightarrow [0, \\infty]$  is a ($p$-to-$e$) calibrator if, for any probability space \n",
    "$(\\Omega, \\mathcal{A}, \\mathbb{P}) and any $P$-variable $P \\in  \\mathcal{P}_\\mathbb{P}$,\n",
    "$f(P) \\in \\mathcal{E}(\\mathcal{P}) \n",
    "\n",
    "A calibrator $f$ *dominates* a calibrator $g$ if $f \\ge g$; $f$ *strictly dominates* $g$ if $f \\ge g$ and $f \\ne g$.\n",
    "A calibrator is *admissible* if it is not strictly dominated by any other calibrator.\n",
    "\n",
    "The following proposition (Vovk & Wang, 2021 Proposition 2.2)\n",
    "says that a calibrator is a nonnegative decreasing\n",
    "function on $[0, 1]$ whose integral is at most 1.\n",
    "\n",
    "**Proposition** . \n",
    "A decreasing function $f : [0, 1] \\rightarrow [0, \\infty]$ is a calibrator if\n",
    "and only if $\\int_0^1 fdp \\le 1$. \n",
    "It is admissible if and only if it is upper semicontinuous,\n",
    "$f(0) = \\infty$, and $\\int_0^1 fdp = 1$.\n",
    "\n",
    "Examples.\n",
    "\n",
    "\\begin{equation}\n",
    "f^\\kappa(p) := \\kappa p^{\\kappa−1}, \\;\\; \\kappa \\in (0, 1).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28119c24-d418-4b0b-a244-69b29bdab4c9",
   "metadata": {},
   "source": [
    "**$E$ to $P$ calibration function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af851a8-061a-4685-b1cf-41932389722a",
   "metadata": {},
   "source": [
    "## Combining $E$-values\n",
    "\n",
    "Suppose $(E_{jt})_{t \\in \\mathbb{N}}$ is an $E$-process for the filtration $(\\mathcal{F}_{jt})$, $j = 1, \\ldots, n$, \n",
    "and let $(\\gamma_{jt})_{t \\in \\mathbb{N}}$ be predictable with respect to $(\\mathcal{F}_{jt})$\n",
    "and satisfy $\\gamma_{jt} \\ge 0$ and $\\sum_{j=1}^n \\gamma_{jt} \\le 1$.\n",
    "Define $\\gamma_t \\cdot E_t := \\sum_{j=1}^n  \\gamma_{jt} E_{jt}$.\n",
    "\\end{equation}\n",
    "Then $(\\gamma_t \\cdot E_t)_{t \\in \\mathbb{N}}$ is an $E$-process.\n",
    "\n",
    "Can adaptively bet more on $E$-processes that are growing large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
