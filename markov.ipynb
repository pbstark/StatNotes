{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower confidence bounds for the mean via Markov's Inequality and methods based on the Empirical Distribution\n",
    "\n",
    "Recall the set-up. We have:\n",
    "\n",
    "1. known: $N > 0$,  $ \\{\\ell_j\\}_{j=1}^N$, and $\\{u_j\\}_{j=1}^N$, with $\\ell_j \\le u_j, \\; \\forall j$\n",
    "2. an unknown population of numbers $\\{x_j\\}_{j=1}^N$ that satisfy $\\ell_j \\le x_j \\le u_j, \\; \\forall j$\n",
    "\n",
    "We want to draw inferences about \n",
    "\n",
    "\\begin{equation*} \\mu := \\frac{1}{N} \\sum_{j=1}^N x_j.\\end{equation*}\n",
    "\n",
    "In this section, $\\ell_j = 0$ and $u_j = \\infty$, $\\forall j$ (the population is known to be nonnegative).\n",
    "\n",
    "We seek a _lower_ confidence bound for $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov's Inequality:\n",
    "\n",
    "\n",
    "If $\\mathbb P(X \\ge 0) = 1$, then $\\mathbb P (X \\ge a) \\le \\mathbb E(X)/a$, $a > 0$.\n",
    "\n",
    "\n",
    "### Confidence bounds from Markov's inequality\n",
    "\n",
    "Suppose we draw a random sample (with or without replacement) of size $n$.  Let $X_j$ be the value of the $j$th draw, and let \n",
    "\n",
    "\\begin{equation*} \\bar{X} := \\frac{1}{n} \\sum_{j=1}^n X_j.\\end{equation*}\n",
    "\n",
    "Then:\n",
    "+ $\\mathbb P(\\bar{X} \\ge 0) = 1$\n",
    "+ $\\mathbb E(\\bar{X}) = \\mu \\ge 0$\n",
    "+ by Markov, for any $a > 0$, $\\mathbb P (\\bar{X} \\ge a) \\le \\mu/a$\n",
    "\n",
    "Hence, for $\\alpha \\in (0, 1)$,\n",
    "\\begin{equation*}\n",
    "  \\mathbb P (\\alpha \\bar{X} \\ge \\mu) \\le \\alpha;\n",
    "\\end{equation*}\n",
    "i.e., $[\\alpha \\bar{X}, \\infty)$ is a lower 1-sided $1-\\alpha$ confidence bound for $\\mu$.\n",
    "\n",
    "A lower confidence bound is possible because the support of $\\mathbb P$ is bounded below; no upper confidence bound for the mean is possible without additional assumptions.\n",
    "\n",
    "Note that this bound does not involve $n$ at all!  \n",
    "It only uses the fact that $\\mathbb P (\\bar{X} \\ge 0) = 1$.\n",
    "Clearly it has a great deal of slack, getting worse as $n$ grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Kolmogorov statistic \n",
    "\n",
    "We now take a completely different approach, based on the empirical cumulative distribution function (ecdf).\n",
    "\n",
    "We draw a uniform sample of size $n$ with replacement from the population $\\{x_j\\}_{j=1}^N$.\n",
    "\n",
    "Let $X_i$ be the $i$th item drawn, $1 \\le i \\le n$.\n",
    "For uniform sampling with replacement, $\\{X_i \\}$ are iid.\n",
    "\n",
    "Define $1_A$ = {1, if $A$; 0, otherwise}.\n",
    "The theoretical cdf of $X_i$ is\n",
    "\\begin{equation*}\n",
    "\tF(x) := \\mathbb P \\{ X_i \\le x \\} .\n",
    "\\end{equation*}\n",
    "\n",
    "Define the empirical cumulative distribution function\n",
    "\\begin{equation*}\n",
    "    \\hat{F}_n (x) := \\frac{1}{n} \\sum_{i=1}^n 1_{x \\ge X_i}\n",
    "\\end{equation*}\n",
    "\n",
    "We shall use the ecdf to construct a confidence set for $F$; from that confidence set we can \n",
    "construct a confidence bound for $\\mathbb E X_1$.\n",
    "\n",
    "Consider the one-sided Kolmogorov-Smirnov statistics\n",
    "\\begin{equation*}\n",
    "\tD_n^+ := \\sup_x (\\hat{F}_n(x) - F(x) )\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{equation*}\n",
    "    D_n^- := \\sup_x (F(x) - \\hat{F}_n(x)),\n",
    "\\end{equation*}\n",
    "and the two-sided version\n",
    "\\begin{equation*}\n",
    "    D_n := \\sup_x |F(x) - \\hat{F}_n(x)|.\n",
    "\\end{equation*}\n",
    "Kolmogorov (1933) and Smirnov (1944) seem to have been the first to study these\n",
    "statistics, which have the same distribution&mdash;a distribution that does not depend on $F$ if $F$ is continuous.\n",
    "\n",
    "Here's a simulation: we take a random sample from a Uniform distribution and plot the ecdf and the true cdf. The ecdf is the stair-step function. In places it is above the true cdf, and in places it is below. $D^-$ measures how far $F$ ever gets above $\\hat{F}_n$. Note that as $n$ increases, the maximum distance between $F$ and $\\hat{F}_n$ tends to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the first cell with code: set up the Python environment\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from scipy.stats import binom\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc186c111aa432d90ddca5646354c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', max=300, min=3), Output()), _dom_classes=('widget-iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plotUniformKolmogorov(n)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ecdf(x):\n",
    "    '''\n",
    "       calculates the empirical cdf of data x\n",
    "       returns the unique values of x in ascending order and the cumulative probabity at those values\n",
    "       NOTE: This is not an efficient algorithm: it is O(n^2), where n is the length of x. \n",
    "       A better algorithm would rely on the Collections package or something similar and could work\n",
    "       in O(n log n)\n",
    "    '''\n",
    "    theVals = sorted(np.unique(x))\n",
    "    theProbs = np.array([sum(x <= v) for v in theVals])/float(len(x))\n",
    "    if (theVals[0] > 0.0):\n",
    "        theVals = np.append(0., theVals)\n",
    "        theProbs = np.append(0., theProbs)\n",
    "    return theVals, theProbs\n",
    "\n",
    "\n",
    "def plotUniformKolmogorov(n):\n",
    "    sam = np.random.uniform(size=n)\n",
    "    v, pr = ecdf(sam)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', color='r', label='true cdf')\n",
    "    ax.step(v, pr, color='b', where='post', label='ecdf')\n",
    "    ax.legend(loc='best')\n",
    "    dLoc = np.argmax(v-pr)\n",
    "    dMinus = (v-pr)[dLoc]\n",
    "    ax.axvline(x=v[dLoc], ymin=pr[dLoc], ymax=v[dLoc], color='g', linewidth='3')\n",
    "    ax.text(0.5, 0.1, r'$D_n^-=$' + str(round(dMinus, 3)), color='g', weight='heavy')\n",
    "    plt.show()\n",
    "\n",
    "interact(plotUniformKolmogorov, n=widgets.IntSlider(min=3, max=300, step=1, value=10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are universal constants $C_n(\\alpha)$ such that for all continuous $F$, \n",
    "\\begin{equation}\n",
    "   \\mathbb{P}_F \\{D_n^- > C_n(\\alpha)\\} = \\alpha.\n",
    "\\end{equation}\n",
    "The value of $C_n(\\alpha)$ involves the Jacobi theta function, but there are bounds that are easy to compute (see below).\n",
    "When $F$ is not necessarily continuous, $C_n(\\alpha)$ is conservative in the sense that\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbb{P}_F \\{D_n^- > C_n(\\alpha)\\} \\ge  \\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "Using the actual value of $C_n(\\alpha)$ yields _Anderson's method_, published as a technical report in 1969.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Massart-Dvoretzky-Kiefer-Wolfowitz (MDKW) Inequality\n",
    "Dvoretsky, Kiefer and Wolfowitz (1956, DKW) showed that\n",
    "\\begin{equation*}\n",
    "\t\\mathbb P_F \\{D_n^- > t\\} \\le C \\exp(- 2 n t^2)\n",
    "\\end{equation*}\n",
    "for some constant $C$.\n",
    "\n",
    "Massart (1990) showed that $C = 1$ is the sharp constant in the DKW inequality, provided \n",
    "$\\exp(-2 n t^2) \\le \\frac{1}{2}$.\n",
    "The distribution of $D_n^-$ is stochastically larger when \n",
    "$F$ is continuous than when $F$ has jumps (Massart 1990); thus the inequality conservative for iid \n",
    "sampling from finite populations.\n",
    "\n",
    "Moreover, $D_n^-$ is stochastically larger for sampling with replacement than for sampling \n",
    "without replacement, so the inequality is conservative for sampling from a finite population\n",
    "without replacement as well.\n",
    "\n",
    "Let's call the inequality with $C=1$ the Massart-Dvoretsky-Kiefer-Wolfowitz (MDKW) inequality.\n",
    "It follows from the MDKW inequality that\n",
    "\\begin{equation*}\n",
    "    \\mathbb P \\left \\{\\sup_x (F(x) - \\hat{F}_n(x)) > \\sqrt{-\\frac{\\ln \\alpha}{2n}} \\right \\} \\le \\alpha.\n",
    "\\end{equation*}\n",
    "That is, \n",
    "\\begin{equation*}\n",
    "    \\mathcal C(X) := \\left \\{ \\mbox{cdfs } G: \\sup_x (G(x) - \\hat{F}_n(x)) \\le  \\sqrt{-\\frac{\\ln \\alpha}{2n}} \\right \\}\n",
    "\\end{equation*}\n",
    "is a nonparametric confidence set for the true population cdf $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Kolmogorov statistic to get a lower confidence bound for the mean of a nonnegative population\n",
    "\n",
    "We now assume all $\\{x_j\\}_{j=1}^N$ are nonnegative, so $F(0^-) = 0$.\n",
    "The population mean is\n",
    "\n",
    "\\begin{equation*}  \n",
    "   \\frac{1}{N} \\sum_{j=1}^N x_j = \\int_0^\\infty x dF(x).\n",
    "\\end{equation*}\n",
    "\n",
    "But with probability at least $1-\\alpha$, $F \\in \\mathcal C(X)$.\n",
    "Hence \n",
    "\n",
    "\\begin{equation*}\n",
    "   \\inf_{G \\in \\mathcal C(X)} \\int_{0}^\\infty x dG(x)\n",
    "\\end{equation*}\n",
    "is a lower $1-\\alpha$ confidence bound for $\\mu = \\int_0^\\infty x dF(x)$.\n",
    "\n",
    "A little more explanation might help: whenever $F \\in \\mathcal C(X)$ (and perhaps more often),\n",
    "\n",
    "\\begin{equation*} \\inf_{G \\in \\mathcal C(X)} \\int_{0}^\\infty x dG(x) \\le \\int_0^\\infty x dF(x).\\end{equation*}\n",
    "But $\\mathbb P (C(X) \\ni F) \\ge 1-\\alpha$.\n",
    "\n",
    "Moreover, we can find confidence bounds for <em>an arbitrary collection of\n",
    "functionals, even an uncountable collection</em> and they will have simultaneous coverage\n",
    "probability $1-\\alpha$: there is no need to adjust for multiplicity, since whenever\n",
    "$F \\in C(X)$, <em>all</em> the bounds hold.\n",
    "\n",
    "This general approach&mdash;make a confidence set for an entire function, then use that\n",
    "confidence set to derive confidence bounds on properties (parameters) of that function&mdash;is\n",
    "extremely flexible, and gives conservative results.\n",
    "\n",
    "### Among all $G \\in \\mathcal C(X)$, which has smallest mean?\n",
    "It's the distribution function that has as much mass as possible as far \"left\" as possible, \n",
    "but never exceeds $\\hat{F}_n$ by more than a tolerance that ensures the CDF is in the\n",
    "confidence set. \n",
    "(We will use the conservative tolerance $\\sqrt{-\\frac{\\ln \\alpha}{2n}}$ from the MDKW inequlity.)\n",
    "\n",
    "Let $G^-$ denote this cdf.\n",
    "\n",
    "Then, \n",
    "\n",
    "\\begin{equation*}\n",
    "   G^-(x) = \\left ( \\hat{F}_n(x) +  \\sqrt{-\\frac{\\ln \\alpha}{2n}} \\right ) \\wedge 1,\n",
    "\\end{equation*} \n",
    "\n",
    "where $a \\wedge b := \\min\\{a, b\\}$.\n",
    "\n",
    "The corresponding lower confidence bound on $\\mu$ is $\\int_0^\\infty x dG^-(x)$.\n",
    "\n",
    "The integral is really a sum because $G^-$ is a step function.\n",
    "Note that $G^-$ might have a jump at $x=0$ even if $\\hat{F}_n$ does not, but the rest of its jumps are \n",
    "at (a subset of) the data. \n",
    "\n",
    "Let's implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find lower confidence bound on the mean of a non-negative population using the MDKW inequality\n",
    "\n",
    "def massartOneSide(n, alpha):\n",
    "    ''' \n",
    "       tolerable misfit between the cdf and ecdf for a one-sided bound \n",
    "       at significance level alpha for an iid sample of size n\n",
    "    '''\n",
    "    return np.sqrt(-np.log(alpha)/(2.0*n))\n",
    "    \n",
    "    \n",
    "def ksLowerMean(x, c):\n",
    "    '''\n",
    "       lower confidence bound for the mean of a nonnegative population\n",
    "       x is an iid sample with replacement from the population\n",
    "       c is the Massart constant for the desired coverage\n",
    "    '''\n",
    "    # find the ecdf\n",
    "    vals, probs = ecdf(x)\n",
    "    probs = np.fmin(probs+c, 1)   # This is G^-\n",
    "    gProbs = np.diff(np.append([0.0], probs))  # pre-pend a 0 so that diff does the right thing; \n",
    "                                               # gProbs is the vector of masses\n",
    "    return (vals*gProbs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this approach to lower one-sided Student-t intervals in a simulation using a nonstandard mixture of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability and expected lower endpoint of one-sided Student-t and MDKW confidence intervals for mixture of U[0,1] and pointmass at 1 population</h3><strong>Nominal coverage probability 95.0%</strong>.<br /><strong>Estimated from 10000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 1</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Student-t cov</th>\n",
       "      <th>MDKW cov</th>\n",
       "      <th>Student-t low</th>\n",
       "      <th>MDKW low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25</td>\n",
       "      <td>89.69%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>0.7048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50</td>\n",
       "      <td>98.85%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100</td>\n",
       "      <td>99.99%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>0.8276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.6623</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25</td>\n",
       "      <td>21.61%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>0.7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50</td>\n",
       "      <td>38.74%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100</td>\n",
       "      <td>61.92%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.8727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400</td>\n",
       "      <td>97.84%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9069</td>\n",
       "      <td>0.9338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25</td>\n",
       "      <td>2.28%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.7548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9939</td>\n",
       "      <td>0.8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100</td>\n",
       "      <td>8.95%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.8771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400</td>\n",
       "      <td>31.81%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>0.9383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 1  sample size Student-t cov MDKW cov Student-t low MDKW low\n",
       "0       0.900           25        89.69%   100.0%        0.6823   0.7048\n",
       "1       0.900           50        98.85%   100.0%        0.6692   0.7769\n",
       "2       0.900          100        99.99%   100.0%        0.6643   0.8276\n",
       "3       0.900          400        100.0%   100.0%        0.6623   0.8889\n",
       "4       0.990           25        21.61%   100.0%        0.9548   0.7502\n",
       "5       0.990           50        38.74%   100.0%         0.943    0.822\n",
       "6       0.990          100        61.92%   100.0%        0.9286   0.8727\n",
       "7       0.990          400        97.84%   100.0%        0.9069   0.9338\n",
       "8       0.999           25         2.28%   100.0%        0.9959   0.7548\n",
       "9       0.999           50         4.81%   100.0%        0.9939   0.8264\n",
       "10      0.999          100         8.95%   100.0%        0.9918   0.8771\n",
       "11      0.999          400        31.81%   100.0%        0.9849   0.9383"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at 1 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "\n",
    "simTable = pd.DataFrame(columns=('mass at 1', 'sample size', 'Student-t cov',\\\n",
    "                                 'MDKW cov', 'Student-t low', 'MDKW low')\n",
    "                       )\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p  # population is a mixture of uniform with mass (1-p) and\n",
    "                             # pointmass at 1 with mass p\n",
    "    \n",
    "    for n in ns:\n",
    "        tCrit = sp.stats.t.ppf(q=1-alpha, df=n-1)\n",
    "        mCrit = np.sqrt(-np.log(alpha)/(2.0*n))  # the 1-sided MDKW constant\n",
    "        coverT = 0\n",
    "        coverM = 0\n",
    "        lowT = 0.0\n",
    "        lowM = 0.0\n",
    "        \n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)  # the uniform part of the sample\n",
    "            ptMass = np.random.uniform(size=n)  # for a bunch of p-coin tosses\n",
    "            sam[ptMass < p] = 1.0   # mix in pointmass at 1, with probability p\n",
    "            # t interval\n",
    "            samMean = np.mean(sam)\n",
    "            samSD = np.std(sam, ddof=1)\n",
    "            tLo = samMean - tCrit*samSD  # lower endpoint of the t interval\n",
    "            coverT += ( tLo <= popMean )\n",
    "            lowT += tLo\n",
    "            #  MDKW interval\n",
    "            mLo = ksLowerMean(sam, mCrit) # lower endpoint of the MDKW interval\n",
    "            coverM += (mLo <= popMean )\n",
    "            lowM += mLo\n",
    "\n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(coverT)/float(reps)) + '%',\\\n",
    "            str(100*float(coverM)/float(reps)) + '%',\\\n",
    "            str(round(lowT/float(reps),4)),\\\n",
    "            str(round(lowM/float(reps),4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lower endpoint of ' +\\\n",
    "          'one-sided Student-t and MDKW confidence intervals for ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 1 population</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>.<br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometimes the average lower bound for the MDKW interval is _larger_ even though it has better coverage. To some extent, that's because we have not truncated the Student-t interval below at 0, which would be legitimate.\n",
    "\n",
    "Still, the length penalty for conservative coverage is modest, especially given that the nominal 95% Student-t interval has coverage below 2% in some of these scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else?\n",
    "\n",
    "The MDKW inequality is not the only way to construct a confidence set for a cdf. One can also use differences of order statistics, among other things.  I have not compared the relative efficiency of such methods."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
