{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf43c57-bc5c-4074-8679-85b726513f36",
   "metadata": {},
   "source": [
    "# Multiple Testing and Simultaneous Confidence Sets\n",
    "\n",
    "References:\n",
    "+ Benjamini, Y., and Y. Hochberg, 1995, Controlling the False Discovery Rate: a Practical and Powerful Approach to Multiple Testing, _JRSS B, 57_, 289-300.\n",
    "+ Benjamini, Y., and D. Yekutieli, 2001. The control of the false discovery rate in multiple testing under dependency, _Ann. Statist., 29_, 1165-1188 https://doi.org/10.1214/aos/1013699998\n",
    "+ Blanchard, G., and E. Roquain, 2008. Two simple sufficient conditions for\n",
    "FDR control, _Electronic Journal of Statistics, 2_,  963–992. DOI: 10.1214/08-EJS180.\n",
    "+ Genovese, C.R., and L. Wasserman, 2004. A stochastic process approach to false discovery control, _Ann. Statist., 32, 1035--1061. https://doi.org/10.1214/009053604000000283\n",
    "+ Hsu, J., 1996. _Multiple Comparisons: Theory and Methods_, Chapman and Hall, London.\n",
    "+ Marcus, R., E. Peritz, and K.R. Gabriel, 1976. On Closed Testing Procedures with Special Reference to Ordered Analysis of Variance, _Biometrika, 63_, 655-660, https://doi.org/10.2307/2335748\n",
    "+ Shaffer, J., 1995. Multiple Hypothesis Testing, _Ann. Rev. Psychol., 46_, 561-584.\n",
    "+ Simes, R.J., 1986. An improved Bonferroni procedure for multiple tests of significance. _Biometrika, 73_, 751–754. https://doi.org/10.1093/biomet/73.3.751\n",
    "+ Wang and Ramdas, 2022. False Discovery Rate Control with E-values, _Journal of the Royal Statistical Society Series B, 84_, 822–852. https://doi.org/10.1111/rssb.12489 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e391dd-968a-4e05-8427-33c7177de717",
   "metadata": {},
   "source": [
    "## What is multiplicity, and why does it matter?\n",
    "\n",
    "If we test hypotheses at significance level $\\alpha$, by definition, the chance we reject a particular null hypothesis if that hypothesis is true is at most $\\alpha$.\n",
    "\n",
    "But often we test not just one, but several or many hypotheses.\n",
    "For example, we might be evaluating a collection of drugs, and want to test the\n",
    "family of null hypotheses that each is not effective.\n",
    "Or we might be evaluating one drug using several tests for different clinical outcomes.\n",
    "\n",
    "Suppose we test each null hypotheses at level $\\alpha$.\n",
    "If we test a collection of hypotheses among which more than one are true, the chance that we erroneously reject one or more true nulls (if more than one is true) may be much larger than the chance of rejecting each individual true null.\n",
    "\n",
    "Similarly, if we make a collection of $1-\\alpha$ confidence sets for $n>1$ parameters, by definition, the chance that each confidence set will contain its corresponding parameter is at least $1-\\alpha$, but the chance that _all $n$_ sets contain their corresponding parameter can be much lower.\n",
    "\n",
    "By analogy, the chance that a fair coin lands heads in a single toss is 1/2, but the chance a fair coin lands heads at least once in 10 independent tosses is $1-(1/2)^{10} > 0.999$.\n",
    "\n",
    "The issue of multiplicity has become more obvious as Statistics has been brought to bear on problems in genetics and genomics, brain imaging, and similar problems, where millions of hypotheses are tested in a single study.\n",
    "\n",
    "There is an enormous literature on multiple testing procedures and simultaneous confidence sets. This chapter presents a tiny slice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c8f99-0b52-479c-8111-0f2ab345a896",
   "metadata": {},
   "source": [
    "## The per-comparison error rate (PCER)\n",
    "\n",
    "The error rate \"per true tested hypothesis\" is called the _per-comparison error rate_ (PCER).\n",
    "If every hypothesis is tested at level $\\alpha$, the PCER is controlled at level $\\alpha$.\n",
    "\n",
    "But the chance of making at least one Type I error is at least $\\alpha$,\n",
    "and is typically larger.\n",
    "\n",
    "The same multiplicity issues arise in computing confidence sets:\n",
    "If $\\{ I_j \\}_{j=1}^m$ are individually level $1-\\alpha$ confidence sets for\n",
    "a collection of parameters $\\{ \\mu_j \\}_{j=1}^m$, \n",
    "so that \n",
    "\\begin{equation}\n",
    "   \\mathbb{P} \\{ I_j \\ni \\mu_j \\} = 1-\\alpha, \\;\\; j = 1, \\ldots, m,\n",
    "\\end{equation}\n",
    "then the event\n",
    "\\begin{equation}\n",
    "A = \\cap_{j=1}^m \\{ I_j \\ni \\mu_j \\}\n",
    "\\end{equation}\n",
    "typically has probability much less than $1-\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930196fb-465f-4012-9f41-a52d74d7f230",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "We'll use the notation in Benjamini and Hochberg (1995).\n",
    "\n",
    "$\\mbox{ }$  | <div style=\"width:200px\">Declared non-significant</div> |  <div style=\"width:200px\">Declared significant</div>| <div style=\"width:150px\">Total</div> |\n",
    ":---- | :------------------: | :-------------: | :-----------: | \n",
    "True null hypotheses | $U$ |  $V$ | $m_0$ | \n",
    "False null hypotheses | $T$ | $S$ | $m - m_0$ |\n",
    "Total | $m-R$ | $R$ |  $m$ |\n",
    "\n",
    "\n",
    "The number of hypotheses tested is $m$, considered to be known.\n",
    "The number of null hypotheses that are true is $m_0$; $m_0$ is unknown.\n",
    "The total number of rejected null hypotheses is the random variable $R$, which is observable.\n",
    "The random variables $U$, $V$, $T$, and $S$ are not observable.\n",
    "\n",
    "If each hypothesis is tested individually at level $\\alpha$, then the PCER is\n",
    "$\\mathbb{E}(V/m) \\le \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad44b2-cea8-4552-96d9-347b12318a6e",
   "metadata": {},
   "source": [
    "## The Familywise Error Rate (FWER)\n",
    "\n",
    "Let $\\{ H_j \\}_{j=1}^m$ ($m$ for multiplicity)\n",
    "be the family of null hypotheses to\n",
    "be tested, and let $H_0 = \\cap_{j} H_j$ be the _grand null hypothesis._\n",
    "If $H_0$ is true, the expected number of rejections is $\\alpha m$.\n",
    "The _familywise error rate_ (FWER) is the probability of one or more incorrect\n",
    "rejections:\n",
    "\\begin{equation}\n",
    "\\mbox{FWER} = \\mathbb{P} \\{ V > 0 \\} = \\mathbb{P} \\left \\{ \\mbox{ reject one or more true } H_j, \\; \\; j \\in \\{1, \\ldots, m \\} \\right \\}.\n",
    "\\end{equation}\n",
    "But what is $\\mathbb{P}$ here?\n",
    "**Strong control** of the FWER at level $\\alpha$\n",
    "means that the probability of one or more\n",
    "incorrect rejections is at most $\\alpha$, no matter which (if any) of the \n",
    "hypotheses $\\{ H_j \\}$ happen to be true.\n",
    "**Weak control** of the FWER at level $\\alpha$ means that the probability of one\n",
    "or more incorrect rejections when the _grand null hypothesis_ $H_0$ is true is\n",
    "at most $\\alpha$:\n",
    "\\begin{equation}\n",
    "\\mathbb{P}_0 \\{ \\mbox{ reject one or more $H_j$ } \\} \\le \\alpha.\n",
    "\\end{equation}\n",
    "The FWER can be much larger than the significance level at which the\n",
    "individual hypotheses are tested.\n",
    "\n",
    "This _multiplicity problem_ is commonly ignored, which tends to\n",
    "make results appear more significant than they really are: the\n",
    "true significance level of the overall test is larger than the reported, \"nominal\" significance\n",
    "level.\n",
    "This problem is exacerbated by \"publication bias\" in favor of positive results.\n",
    "If only results that are statistically significant are considered worthy of publication,\n",
    "then many tests may be performed in searching for one that rejects the null: the chance that\n",
    "a true null is (eventually) rejected can be very large.\n",
    "Because the non-rejections are \"invisible\" (not published), it's hard to know how many tests were\n",
    "done before the test that led to rejection.\n",
    "\n",
    "### Closed testing procedures\n",
    "\n",
    "Primary reference:  Marcus et al. (1976).\n",
    "\n",
    "One way to control the FWER is to use _closed testing procedures_, which test a collection of hypotheses that is closed \n",
    "under intersection.\n",
    "That is, if the (possibly compound) hypotheses $H_1$ and $H_2$ are in the collection, so is the hypothesis\n",
    "$H_1 \\cap H_2$.\n",
    "\n",
    "We have a set of $K$ composite null hypotheses, $\\{H_k \\}_{k=1}^K$.\n",
    "We want to test all $K$ hypotheses in such a way that the chance of erroneously rejecting one or more true nulls is \n",
    "at most $\\alpha$; that is, we want the FWER to be at most $\\alpha$.\n",
    "\n",
    "Let $\\mathcal{H}$ denote the set of all intersections of subsets of $\\{H_k\\}$.\n",
    "The _closure principle_ says that if we test as follows, it controls the FWER.\n",
    "Test every (composite) null in $\\mathcal{H}$ at level $\\alpha$.\n",
    "Reject a null $H_0 \\in \\mathcal{H}$ only if its test and the \n",
    "test of every hypothesis in $\\mathcal{H}$ that is a subset of $H_0$ all reject.\n",
    "\n",
    "\n",
    "The proof that this procedure controls FWER at level $\\alpha$ is simple. \n",
    "Let $A$ be the event that any _true_ $H_k$ is rejected.\n",
    "Let $B$ be the event that the intersection of all the true nulls is rejected (that _every_ true null is rejected).\n",
    "Because $\\mathcal{H}$ is closed under intersections, the intersection of all true nulls is one of the hypotheses in $\\mathcal{H}$.\n",
    "Then\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{A \\cap B \\} = \\mathbb{P}(A | B) \\mathbb{P}(B) \\le \\alpha\n",
    "\\end{equation}\n",
    "since the intersection is tested at level $\\alpha$.\n",
    "Clearly $B \\subset A$, so $A \\cap B = B$.\n",
    "But since this procedure will only reject a true null if it has also rejected the intersection of all\n",
    "true nulls, $\\{A \\cap B \\} = A$, and hence $\\mathbb{P}(A) \\le \\alpha$.\n",
    "Thus closed testing limits the FWER to $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab754140-0c21-49ea-a913-d9257decf8f5",
   "metadata": {},
   "source": [
    "### Procedures based on Bonferroni's Inequality\n",
    "\n",
    "Bonferroni's inequality (also known as the _union bound_) says that\n",
    "for any collection of events $\\{A_j\\}$,\n",
    "$\\mathbb{P} \\{ \\cup_j A_j \\} \\le \\sum_j \\mathbb{P} A_j$.\n",
    "A consequence is that the chance of one or more type I errors in an arbitrary collection of\n",
    "tests is at most the sum of their separate chances of type I errors.\n",
    "Thus if each hypothesis is tested at \n",
    "level $\\alpha/m$, the FWER is\n",
    "$\\mathbb{P}\\{V \\ge 1\\} \\le \\alpha$.\n",
    "\n",
    "This _Bonferroni adjustment_ gives strong control of the FWER, but the resulting tests can be very \n",
    "conservative--unnecessarily so.\n",
    "There are other methods that rely on the union bound in more subtle ways to give more powerful testing\n",
    "procedures that also control FWER.\n",
    "\n",
    "\n",
    "_Holm's Sequentially Rejective Bonferroni Method_ is based on the ordered $P$-values\n",
    "$P_{(1)} \\le P_{(2)} \\le \\cdots \\le P_{(m)}$ of the $m$ hypotheses.\n",
    "Order the hypotheses in the same way, so that the $P$-value of $H_{(1)}$ is $P_{(1)}$, etc.\n",
    "Holm's procedure is \n",
    "\n",
    "> Reject $H_{(i)}$ if $P_{(k)} \\le \\alpha/(m-k+1)$ for all $k \\le i$.  \n",
    "Do not reject the other hypotheses.\n",
    "\n",
    "\n",
    "**Theorem.**  \n",
    "Holm's method controls the FWER at level $\\alpha$.\n",
    "\n",
    "**Proof.**  \n",
    "Let $m_0$ be the number of true null hypotheses.\n",
    "+ If $m_0 = m$, there is an incorrect rejection only if $P_{(1)} \\le \\alpha/m$, which has probability at most $\\alpha$, by Bonferroni's inequality.\n",
    "+ If $m_0 = m-1$, there is an incorrect rejection if either $H_{(1)}$ is one of the true null hypotheses and $P_{(1)} \\le \\alpha/M$, \n",
    "or if $H_{(1)}$ is false, $P_{(1)} \\le \\alpha/m$, and $P_{(2)} \\le \\alpha/(m-1)$. \n",
    "Let $P'_j$ be the $j$th smallest $P$-value among the $m_0$ true null hypotheses. \n",
    "There can only be an incorrect rejection if $P'_1 \\le \\alpha/(m-1)$ (but that condition is not sufficient for an incorrect rejection). By Bonferroni's ineuality, the chance of an incorrect rejection is thus at most $\\alpha$.\n",
    "+ One can proceed similarly for $m_0 = m-2, \\ldots, 1$, arguing that an incorrect rejection can only occur (but does not necessarily occur) if $P'_1 \\le \\alpha/m_0$; in each case, the chance of that event is at most $\\alpha$, by Bonferroni's inequality. \n",
    "+ If $m_0 = 0$, there can be no incorrect rejection.\n",
    "\n",
    "Holm's method is an example of a _step-down procedure_.\n",
    "The schematic of a step-down procedure is that one looks at the smallest $P$-value first.\n",
    "If that is larger than some threshold, no hypothesis is rejected. \n",
    "If not, the corresponding hypothesis is rejected, and one goes on to the second-smallest $P$-value.\n",
    "As soon as one reaches the point that the $j$th smallest $P$-value is larger than the\n",
    "$j$th threshold, no more hypotheses are rejected.\n",
    "\n",
    "In a _step-up procedure_, one looks first at the largest $P$-value. If that is sufficiently\n",
    "small, all the hypotheses are rejected. If not, the corresponding hypothesis is\n",
    "not rejected, and one goes on to the second largest $P$-value. As soon as one reaches\n",
    "the point that the $j$th largest $P$-value is smaller than the $j$th threshold,\n",
    "all the remaining hypotheses are rejected. \n",
    "\n",
    "\n",
    "### Independent Test Statistics\n",
    "Suppose we wish to test with FWER not exceeding $\\alpha$\n",
    "the family of hypotheses $\\{H_i \\}_{i=1}^m$ using independent test statistics\n",
    "$\\{T_i\\}_{i=1}^m$.\n",
    "Suppose we test each hypothesis at level $\\beta$.\n",
    "Then the probability of one or more incorrect rejections (the FWER) is\n",
    "$1- (1-\\beta)^m$. To have FWER equal to $\\alpha$ requires\n",
    "\\begin{eqnarray}\n",
    "\\alpha &=& 1- (1-\\beta)^m \\nonumber \\\\\n",
    "(1-\\alpha )^{1/m} &=& 1 - \\beta \\nonumber \\\\\n",
    "\\beta &=& 1 - (1-\\alpha)^{1/m} .\n",
    "\\end{eqnarray}\n",
    "Thus if we test the hypotheses individually at level $1 - (1-\\alpha)^{1/m}$, the\n",
    "FWER is at most $\\alpha$.\n",
    "This is approach is called _Šidák's adjustment_.\n",
    "\n",
    "### Simes' inequality.\n",
    "\n",
    "See R.J. Simes (1986).\n",
    "\n",
    "Suppose we are testing $m$ null hypotheses $\\{ H_j \\}$ using independent\n",
    "test statistics $T_j$.\n",
    "Let $P_{(j)}$ be the $j$th smallest $P$-value among the $m$ $P$-values.\n",
    "Simes' method is\n",
    "\n",
    "> reject the grand null hypothesis if for some $j$, $P_{(j)} \\le j\\alpha/m$.\n",
    "\n",
    "Simes' method has FWER at most $\\alpha$.\n",
    "\n",
    "**Theorem.** (Simes, 1986)  \n",
    "Let $P_{(j)}$ be the $j$th order statistic of $m$ IID $U(0,1)$ random variables.\n",
    "Then for $\\alpha \\in [0, 1]$,\n",
    "\\begin{equation}\n",
    "A_m(\\alpha) := \\mathbb{P} \\{ P_{(j)} > j\\alpha/m, \\; \\; j = 1, \\ldots, m \\} = 1-\\alpha .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Proof.**  \n",
    "The proof is by induction on $m$.  Clearly, the statement is true for $m=1$: the chance a $U[0, 1]$ \n",
    "random variable is\n",
    "greater than $\\alpha$ is $1-\\alpha$, and more generally, the chance (under the null) that a genuine $P$-value is\n",
    "greater than $\\alpha$ is at least $1-\\alpha$.\n",
    "For $m > 1$, $\\{ P_{(1)}/P_{(m)}, P_{(2)}/P_{(m)}, \\ldots, P_{m-1}/P_m \\}$ are distributed as the\n",
    "order statistics of $m-1$ iid $U(0,1)$ random variables, independent of $P_{(m)}$.\n",
    "Thus, for $p \\ge \\alpha$,\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{P} \\left \\{ P_{(j)} > \\frac{j\\alpha}{m};\\; j = 1, \\ldots, m-1 | P_{(m)} = p \\right \\} \n",
    "&=&\n",
    "\\mathbb{P} \\left \\{ P_{(j)}/p > \\frac{j\\alpha (m-1)}{pm(m-1)} ;\\; j = 1, \\ldots, m-1 | P_{(m)} = p \\right \\}\n",
    "\\nonumber \\\\\n",
    "&=&\n",
    "\\mathbb{P} \\left \\{ P_{(j)} > \\frac{j \\frac{(m-1)\\alpha}{pm}}{m-1};\\; j = 1, \\ldots, m-1 | P_{(m)} = p \\right \\}\n",
    "\\nonumber \\\\\n",
    "&=&\n",
    "A_{m-1} \\left ( \\frac{(m-1)\\alpha}{pm} \\right) .\n",
    "\\end{eqnarray}\n",
    "The distribution function of $P_{(m)}$ is $p^m$, $p \\in [0, 1]$, so the density\n",
    "of $P_{(m)}$ is $mp^{m-1}$.\n",
    "Suppose $A_{m-1}(\\alpha) = 1-\\alpha$, $\\alpha \\in [0, 1]$.\n",
    "\\begin{eqnarray}\n",
    "A_m(\\alpha) &=& \\int_\\alpha^1 A_{m-1} \\left ( \\frac{(m-1)\\alpha}{pm} \\right ) \n",
    "mp^{m-1} dp  \\nonumber \\\\\n",
    "&=& \n",
    "\\int_\\alpha^1 \\left (1- \\frac{(m-1)\\alpha}{pm} \\right ) mp^{m-1} dp  \\nonumber \\\\\n",
    "&=& \n",
    "\\left . p^m \\right |_\\alpha^1 -\n",
    "\\left . \\alpha p^{m-1} \\right |_\\alpha^1\n",
    "\\nonumber \\\\\n",
    "&=& 1 - \\alpha^m - \\alpha + \\alpha^m\n",
    "\\nonumber \\\\\n",
    "&=& 1 - \\alpha .\n",
    "\\end{eqnarray}\n",
    "\n",
    "Because $j \\alpha/m$ is at least as large as $\\alpha/(m-j+1)$,\n",
    "the grand null is rejected more frequently using this test than using Holm's Bonferroni-based test.\n",
    "\n",
    "Simes' result has been generalized to \n",
    "_positively regression dependent test statistics_, defined as follows.\n",
    "Two random variables $X$ and $Y$ are _positively regression dependent_ if\n",
    "for $x_0 < x_1$, a random variable that has the\n",
    "conditional distribution of $Y$ given $X=x_1$ is stochastically\n",
    "larger than that of one with the conditional distribution of $Y$ given $X=x_0$\n",
    "($Y$ tends to be larger when $X$ is larger).\n",
    "Positively correlated normal random variables have positive regression dependence.\n",
    "\n",
    "\n",
    "\n",
    "### Chebychev's Other Inequality\n",
    "\n",
    "**Theorem.** (J. Hsu, 1996, _Multiple Comparisons: Theory and Methods_  Theorem A.1.1.)  \n",
    "Let $X$ be an $n$-dimensional random variable. Suppose the functions $f, g : \\Re^n \\rightarrow\n",
    "\\Re$ satisfy\n",
    "\\begin{equation}\n",
    "[f(x_2) - f(x_1)][g(x_2) - g(x_1)] \\ge 0\n",
    "\\end{equation}\n",
    "for all $x_1, x_2$ in the support of the distribution of $X$.\n",
    "Then, provided the expectations exist,\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[f(X)g(X)] \\ge \\mathbb{E}[f(X)]\\mathbb{E}[g(x)] .\n",
    "\\end{equation}\n",
    "I.e., $f(X)$ and $g(X)$ are positively correlated.\n",
    "\n",
    "\n",
    "\n",
    "**Proof.**  \n",
    "Let $X$, $X_1$ and $X_2$ be iid. \n",
    "Then\n",
    "\\begin{eqnarray}\n",
    "0 &\\le& \\mathbb{E} \\left [ (f(X_2) - f(X_1) )(g(X_2) - g(X_1) ) \\right ] \\nonumber \\\\\n",
    "&=& \\mathbb{E} \\left [ (f(X_2)g(X_2) + f(X_1)g(X_1)) - (f(X_1)g(X_2) + f(X_2)g(X_1)) \\right ]\n",
    "\\nonumber \\\\\n",
    "&=& 2 \\left [ \\mathbb{E}[f(X)g(X)] - \\mathbb{E}[f(X)]\\mathbb{E}[g(X)] \\right ] .\n",
    "\\end{eqnarray}\n",
    "\n",
    "**Corollary.** (Kimball's inequality; Hsu (1996) Corollary A.1.1.)  \n",
    "Let $V$ be a univariate random variable. \n",
    "If $\\{g_j\\}_{j=1}^m$ are bounded, nonnegative\n",
    "real functions, monotone in the same direction, then\n",
    "\\begin{equation}\n",
    "\\mathbb{E} \\left [ \\prod_{j=1}^m g_j(V) \\right ] \\ge \\prod_{j=1}^m \\mathbb{E}g_j(V).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Proof.**  \n",
    "Use induction from two functions to $m$ functions in the Theorem, \n",
    "taking $n=1$.\n",
    "\n",
    "### Application to the one-way model\n",
    "Suppose we are interested in the _one-way model_.\n",
    "We observe\n",
    "\\begin{equation}\n",
    "X_{ia} = \\mu_i + \\epsilon_{ia}, \\;\\; i=1, \\ldots, m; \\; a = 1, \\ldots, n_i .\n",
    "\\end{equation}\n",
    "This is a model for making $n_i$ observations of the response to treatment $i$\n",
    "for $m$ different treatments, under the assumption that the response is a mean\n",
    "response plus a random effect.\n",
    "Assume that the errors $\\epsilon_{ia}$ are iid $N(0, \\sigma^2)$, $\\sigma^2$ unknown.\n",
    "Let $\\hat{\\mu}_i = \\bar{X}_i = \\frac{1}{n_i} \\sum_{a=1}^{n_i} X_{ia}$.\n",
    "Let $\\nu = \\sum_{i=1}^m (n_i - 1)$, and define\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}^2 = \n",
    "\\frac{1}{\\nu}\\sum_{i=1}^m \\sum_{a=1}^{n_i} (X_{ia} - \\bar{X}_i)^2 .\n",
    "\\end{equation}\n",
    "The estimators $\\{\\hat{\\mu}_i\\}$ are independent normals with means $\\{ \\mu_i \\}$\n",
    "and variances $\\{ \\sigma^2/n_i \\}$, independent of $\\hat{\\sigma}^2$,\n",
    "and $\\nu \\hat{\\sigma}^2/\\sigma^2 \\sim \\chi^2_\\nu$.\n",
    "For future use, define\n",
    "\\begin{equation}\n",
    "\\hat{\\mu} = (\\hat{\\mu}_j)_{j=1}^m.\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\hat{\\sigma}_B^2 = \\frac{1}{m} \\sum_{i=1}^m n_i \\left ( \\hat{\\mu}_i - \n",
    "\\frac{1}{m}\\sum_{i=1}^m \\hat{\\mu}_i \\right )^2.\n",
    "\\end{equation}\n",
    "\n",
    "Suppose we wish to find simultaneous confidence intervals for the\n",
    "set of parameters $\\{ \\mu_i \\}$.\n",
    "\n",
    "Define the Studentized test statistics \n",
    "\\begin{equation}\n",
    "T_i = \\frac{\\hat{\\mu}_i - \\mu_i}{\\hat{\\sigma}/\\sqrt{n_i}}, \\; i=1, \\ldots, m .\n",
    "\\end{equation}\n",
    "These test statistics are dependent, because of the common divisor $\\hat{\\sigma}$.\n",
    "If they were not, the intervals\n",
    "\\begin{equation}\n",
    "[ \\hat{\\mu}_i - \\hat{\\sigma} t_{1- ( 1- ( 1-\\alpha/2)^{1/m})/2},\n",
    "\\hat{\\mu}_i + \\hat{\\sigma} t_{1- ( 1- ( 1-\\alpha/2)^{1/m})/2}],\n",
    "\\; i = 1, \\ldots, m\n",
    "\\end{equation}\n",
    "would be exact $1-\\alpha$ simultaneous confidence intervals for $\\{ \\mu_i \\}$.\n",
    "\n",
    "Kimball's inequality lets one show that these intervals are in fact conservative as\n",
    "a result of the dependence on $\\hat{\\sigma}$.\n",
    "Let $A_i$ be the event that the $i$th interval covers. Consider the function\n",
    "$g_i(\\hat{\\sigma}) = \\mathbb{P} \\{ A_i | \\hat{\\sigma} \\}$.\n",
    "These functions all increase monotonically with $\\hat{\\sigma}$.\n",
    "Recall that $\\{\\hat{\\mu}_i\\}$ are independent of each other and of $\\hat{\\sigma}$.\n",
    "Thus\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{P} \\left \\{ \\cap_{i=1}^m A_i \\right \\} &=& \\mathbb{E} 1_{\\cap_{i=1}^m A_i}\n",
    "\\nonumber \\\\\n",
    "&=& \\mathbb{E} \\Pi_{i=1}^m 1_{A_i} \\nonumber \\\\\n",
    "&=& \\mathbb{E}_{\\hat{\\sigma}} \\mathbb{E} (\\Pi_{i=1}^m 1_{A_i} | \\hat{\\sigma} )\n",
    "\\nonumber \\\\\n",
    "&=& \\mathbb{E}_{\\hat{\\sigma}} \\Pi_{i=1}^m \\mathbb{P} \\{ A_i | \\hat{\\sigma} \\}\n",
    "\\nonumber \\\\\n",
    "&\\ge& \\Pi_{i=1}^m \\mathbb{E}_{\\hat{\\sigma}} \\mathbb{P} \\{ A_i | \\hat{\\sigma} \\}\n",
    "\\nonumber \\\\\n",
    "&=& \\Pi_{i=1}^m \\mathbb{P} \\{ A_i \\} \\nonumber\n",
    "\\\\\n",
    "&=& 1 - \\alpha,\n",
    "\\end{eqnarray}\n",
    "where Kimball's inequality was used in the penultimate step.\n",
    "\n",
    "\n",
    "### Comparisons and Constrasts\n",
    "We specialize to the case that we are interested in a collection of $m$ parameters\n",
    "$\\{ \\mu_i \\}_{i=1}^m$.  \n",
    "Let $\\mu = ( \\mu_i )_{i=1}^m$.\n",
    "The hypotheses we wish to test involve comparing the\n",
    "parameters or linear combinations of the parameters.\n",
    "For example, we might be interested in the \n",
    "family of hypotheses $\\{ H_{ij}: \\mu_i = \\mu_j \\}_{i= 1, \\ldots,\n",
    "m-1; j = i+1, \\ldots, m }$ (all pairwise comparisons).\n",
    "For $\\mathcal{I}$ a subset of $\\{ 1, \\ldots, m \\}$, let\n",
    "$H_{\\mathcal{I}}$ denote the hypothesis that all $\\{ \\mu_i \\}_{i \\in \\mathcal{I}}$ are equal\n",
    "(perhaps a better notation would be that $\\#\\{ \\mu_i \\}_{i \\in \\mathcal{I}} = 1$).\n",
    "We might be interested in the family of hypotheses $\\{ H_{\\mathcal{I}} \\}_{\\mathcal{I} \\in {\\bf \\mathcal{I}}}$,\n",
    "where ${\\bf \\mathcal{I}}$ is a collection of subsets of $\\{ 1, \\ldots, m \\}$.\n",
    "\n",
    "A _contrast_ is a linear combination $\\sum_{i=1}^m c_i \\mu_i = c \\cdot \\mu$, with\n",
    "the restriction that $\\sum_{i=1}^m c_i = c \\cdot {\\mathbf{1}} = 0$. \n",
    "A _pairwise comparison_ is a constrast with $c_i = 1$ for some $i$, $c_j = -1$ for some $j \\ne i$, and all other\n",
    "components of $c$ equal to zero.\n",
    "\n",
    "We are going to assume that we are in a one-way model with and equal number\n",
    "$N$ of observations of each of the $m$ treatments. \n",
    "We again assume that the observational errors are iid $N(0, \\sigma^2)$, with $\\sigma^2$ unknown.\n",
    "The rest of the notation is as above.\n",
    "\n",
    "The _cost_ in terms of reduced power tends to increase with the number of hypotheses\n",
    "tested; if one is not interested in testing all possible contrasts, one can have\n",
    "more power testing the limited set. Some major divisions of families of hypotheses tested\n",
    "in the one-way model include, in decreasing order of complexity,\n",
    "ACC (all contrasts comparison), MCA (all pairwise comparisons), MCB (multiple comparisons\n",
    "with the [sample] best), and MCC (multiple comparisons with control).\n",
    "MCC involves the fewest comparisons: $m-1$ sample values are compared with the $m$th,\n",
    "which is the control. In MCB, there are also only $m-1$ comparisons, but the measured\n",
    "effect $\\hat{\\mu}_i$\n",
    "that the other $m-1$ are compared with is that one observed to be best; under the \n",
    "grand null, that is equally likely to be any of the $\\hat{\\mu}_i$.\n",
    "In MCA, there are $(m^2 - m)/2$ hypotheses tested, and in ACC, an infinite number are\n",
    "tested.\n",
    "\n",
    "### The Scheffé Method\n",
    "The Scheffé method controls the FWER for all possible contrasts (ACC).\n",
    "The _grand null_ in this case is that all the $\\mu_i$ are equal, so all\n",
    "contrasts are zero.\n",
    "\n",
    "Recall that if $Y$ has a chi-square distribution with $k$ degrees of freedom\n",
    "and $Y'$ has a chi-square distribution with $\\ell$ degrees of freedom, and\n",
    "$Y$ and $Y'$ are independent, then\n",
    "\\begin{equation}\n",
    "\\frac{Y/k}{Y'/\\ell}\n",
    "\\end{equation}\n",
    "has an $F$ distribution with $k$ and $\\ell$ degrees of freedom, denoted $F_{k,\\ell}$.\n",
    "Let $F_{k,\\ell,\\alpha}$ denote the $\\alpha$ critical value of $F_{k,\\ell}$.\n",
    "It is a standard result in the analysis of variance that under the one-way normal model,\n",
    "$(m-1)\\hat{\\sigma}_B^2/\\sigma^2 \\sim \\chi_{m-1}^2$ and \n",
    "$\\nu\\hat{\\sigma}^2/\\sigma^2 \\sim \\chi_\\nu^2$ are independent, so\n",
    "\\begin{equation}\n",
    "\\frac{\\hat{\\sigma}_B^2}{\\hat{\\sigma}^2} \\sim F_{m-1,\\nu}.\n",
    "\\end{equation}\n",
    "\n",
    "The variables $\\{ \\sqrt{n_i}(\\hat{\\mu}_i - \\mu_i)/\\sigma \\}$ are iid $N(0,1)$,\n",
    "and $\\sigma^{-2} \\sum_{i=1}^m n_i (\\hat{\\mu}_i - \\mu_i)^2$ has a chi-square distribution\n",
    "with $m$ degrees of freedom, and is independent of $\\hat{\\sigma}^2$,\n",
    "so whatever be $\\{ \\mu_i \\}_{i=1}^m$,\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\left \\{ \n",
    "\\frac{\\sum_{i=1}^m n_i | \\hat{\\mu}_i - \\mu_i |^2}{m \\hat{\\sigma}^2}\n",
    "\\le F_{m,\\nu,\\alpha} \\right \\} = 1-\\alpha .\n",
    "\\end{equation}\n",
    "Equivalently,\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{ \\sum_{i=1}^m n_i|\\hat{\\mu}_i - \\mu_i|^2 \\le m \\hat{\\sigma}^2\n",
    "F_{m,\\nu,\\alpha} \\} = 1-\\alpha .\n",
    "\\end{equation}\n",
    "In the case all $n_i = N$, this becomes\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{ \\| \\hat{\\mu} - \\mu \\|^2 \\le \\frac{m}{N} \\hat{\\sigma}^2\n",
    "F_{m,\\nu,\\alpha} \\} = 1-\\alpha .\n",
    "\\end{equation}\n",
    "That is, the chance is at least $1-\\alpha$ that $\\hat{\\mu} \\in \\Re^m$ \n",
    "is in a ball centered at $\\mu$ of radius \n",
    "\\begin{equation}\n",
    "r_\\alpha = \\frac{m}{N} \\hat{\\sigma} \\sqrt{\\frac{m F_{m,\\nu,\\alpha}}{N}}.\n",
    "\\end{equation}\n",
    "The unit ball in $\\Re^m$ can be characterized as\n",
    "\\begin{equation}\n",
    "\\{ \\beta \\in \\Re^m : |c \\cdot \\beta| \\le \\|c\\| \\},\n",
    "\\end{equation}\n",
    "so \n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\{ |c \\cdot \\hat{\\mu} - c \\cdot \\mu | \\le \\|c\\| r_\\alpha \\; \\forall c \\in \\Re^m \\}\n",
    "= 1-\\alpha .\n",
    "\\end{equation}\n",
    "This gives simultaneous confidence intervals for $c \\cdot \\mu$ (whether or not $c$ is a\n",
    "_contrast_) as\n",
    "\\begin{equation}\n",
    "\\mathcal{I}_c = [ c \\cdot \\hat{\\mu} - \\|c\\|r_\\alpha, c \\cdot \\hat{\\mu} + \\|c\\|r_\\alpha ].\n",
    "\\end{equation}\n",
    "For testing contrasts, one rejects the hypothesis that $c \\cdot \\mu = 0$ if\n",
    "$|c \\cdot \\hat{\\mu}| > \\|c\\| r_\\alpha$, and one rejects the grand null hypothesis\n",
    "if $\\| \\hat{\\mu} \\| \\ge r_\\alpha$.\n",
    "Any number of contrasts can be tested this way, with FWER strongly controlled at \n",
    "level $\\alpha$.\n",
    "\n",
    "Note that if one uses Scheffé's method to produce confidence intervals only for \n",
    "the effects $\\{ \\mu_i \\}$, it is unnecessarily conservative: it amounts to projecting\n",
    "a ball onto the coordinate axes, which is equivalent to taking the corresponding\n",
    "hyperrectangle as the confidence set for $\\mu$. That hyperrectangle strictly contains\n",
    "the ball, so it has higher coverage probability than the ball. \n",
    "If we were interested only in simultaneous confidence intervals for $\\{ \\mu_i\\}$,\n",
    "we could get shorter confidence intervals by starting with a hyperrectangular \n",
    "confidence region for $\\mu$ (with faces aligned with the axes), and projecting _that_\n",
    "set.  This is more or less what Tukey's maximum modulus method does.\n",
    "\n",
    "### Tukey's Maximum Modulus Method\n",
    "\n",
    "Tukey's method was originally introduced for all pairwise comparisons, but can\n",
    "be modified for ACC.\n",
    "Again, let's take $n_i = N$.\n",
    "Define $c^*(\\alpha)$ to satisfy\n",
    "\\begin{equation}\n",
    "\\mathbb{P} \\left \\{ \\frac{|\\hat{\\mu}_i - \\hat{\\mu_j} - ( \\mu_i - \\mu_j ) |}{\\hat{\\sigma}\\sqrt{2/N}}\n",
    "\\le c^*(\\alpha) \\forall j < i \\right \\} = 1-\\alpha.\n",
    "\\end{equation}\n",
    "Values of $c^*(\\alpha)$ can be found by numerical integration.\n",
    "Then\n",
    "\\begin{equation}\n",
    "\\mathcal{I}_{ij} = [\\hat{\\mu}_i - \\hat{\\mu}_j - c^*(\\alpha) \\hat{\\sigma}\\sqrt{2/N},\n",
    "\\hat{\\mu}_i - \\hat{\\mu}_j - c^*(\\alpha) \\hat{\\sigma}\\sqrt{2/N}], \\; j < i\n",
    "\\end{equation}\n",
    "are simultaneous level $1-\\alpha$ confidence intervals for the $(m^2 - m)$ pairwise\n",
    "difference $\\mu_i - \\mu_j$, $j < i$.\n",
    "By construction, the tests\n",
    "> reject $H_{ij}: \\mu_i = \\mu_j$ if $|\\hat{\\mu}_i - \\hat{\\mu}_j| > c^*(\\alpha) \\hat{\\sigma}\\sqrt{2/N}$\n",
    "control the FWER for all pairwise comparisons at level $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0d59c-84ba-486c-adf7-27c514b48d70",
   "metadata": {},
   "source": [
    "## The False Discovery Rate\n",
    "See Benjamini and Hochberg (1995) and Benjamini and Yekutieli (2001).\n",
    "\n",
    "Rejecting a null hypothesis is sometimes called a _statistical discovery_,\n",
    "so rejecting a trull null hypothesis is a _false discovery_. \n",
    "The _False Discovery Rate_ (FDR) of a testing procedure is the expected fraction of rejected null hypotheses that were indeed false. \n",
    "That is, let $V$ denote the number of incorrectly rejected null hypotheses and let $R$ denote the total \n",
    "number of rejected null hypotheses, and define\n",
    "\\begin{equation}\n",
    "  \\mbox{FDR} := \\mathbb{E}(V/R | R>0) \\mathbb{P}(R>0) = \\mathbb{E} Q,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "Q := \\left \\{ \\begin{array}{ll}\n",
    "        V/R, & R>0 \\\\\n",
    "        0, & R=0.\n",
    "        \\end{array}\n",
    "        \\right .\n",
    "\\end{equation}\n",
    "is the _false discovery proportion_ (FDP), a random variable.\n",
    "That is, the FDR is the expected FDP.\n",
    "(The conditioning is to prevent division by zero.\n",
    "The definition of $Q$ makes sense because if no hypothesis was rejected, no hypothesis was rejected\n",
    "erroneously.)\n",
    "\n",
    "1. If all null hypotheses are true, the FDR is the same as the FWER: $\\mathbb{P} \\{V \\ge 1 | m_0 = m\\} = \\mathbb{E}(Q)$. Controlling the FDR thus controls the FWER in a weak sense.\n",
    "1. When the number $m_0$ of true null hypotheses is less than the total number $m$ of hypotheses, FDR $\\le$ FWER. Thus controlling the FWER controls the FDR.\n",
    "\n",
    "### The Benjamini-Hochberg and Benjamini-Yekutieli procedures to control the FDR\n",
    "\n",
    "The Benjamini-Hochberg procedure for testing $m$ hypotheses at FDR level $\\alpha$ is as follows:\n",
    "Let $\\{P_1, \\ldots, P_m\\}$ be the $P$-values of the $m$ hypotheses, \n",
    "and let $\\{P_{(1)}, \\ldots, P_{(m)}\\}$\n",
    "be the $P$-values ordered so that $P_{(1)} \\le \\cdots \\le P_{(m)}$.\n",
    "\n",
    "+ Find $K(\\alpha) := \\max \\left \\{k \\in \\{1, \\ldots, m\\} : P_{(k)} \\le \\frac{k}{m} \\alpha \\right \\}$ (with $\\max \\emptyset := 0$).\n",
    "+ Reject the null hypotheses $H_{(i)}$ for all $i \\le K(\\alpha)$.\n",
    "\n",
    "This procedure (the B-H procedure) works for independent $P$-values and $P$-values that have \n",
    "_positive regression dependence on a subset_,\n",
    "defined as follows:\n",
    "A set $D \\subset \\Re^n$ is _increasing_ if $x \\in D$ and $y \\ge x$ implies $y \\in D$.\n",
    "An $m$-vector $X$ of $P$-values is PRDS if for every component $X_k$ corresponding to a true null $H_k$ \n",
    "and every increasing set $D \\subset \\Re^m$,\n",
    "\\begin{equation}\n",
    " \\mathbb{P} \\{ X \\in  D | X_k \\le x \\}\n",
    "\\end{equation}\n",
    "is an increasing function of $x$.\n",
    "\n",
    "\n",
    "The B-H procedure guarantees that $\\mbox{FDR} \\le \\frac{m_0}{m}\\alpha \\le \\alpha$, where $m_0$ is the number of true null\n",
    "hypotheses.\n",
    "The Benjamini-Yekutieli procedure works for arbitrary dependence. It involves an additional\n",
    "function $c(m):= \\sum_{i=1}^m 1/i$:\n",
    "\n",
    "+ Find $K^*(\\alpha) := \\max \\left \\{k \\in \\{1, \\ldots, m\\} : P_{(k)} \\le \\frac{k}{mc(m)} \\alpha \\right \\}$ (with $\\max \\emptyset := 0$).\n",
    "+ Reject the null hypotheses $H_{(i)}$ for all $i \\le K^*(\\alpha)$.\n",
    "\n",
    "\n",
    "**Theorem.** Benjamini and Hochberg (1995) Theorem 1.  \n",
    "If the test statistics are independent, then for any configuration of\n",
    "false null hypotheses, the Benjamini-Hochberg procedure controls the FDR at level $\\alpha$.\n",
    "\n",
    "The proof relies on the following lemma:\n",
    "\n",
    "**Lemma.**  Benjamini and Hochberg (1995)  \n",
    "Let the number of true null hypotheses be $m_0$, $0 \\le m_0 \\le m$.\n",
    "Order the hypotheses such that the first $m_0$ are the true ones.\n",
    "Let $m_1 = m - m_0$ be the number of false null hypotheses.\n",
    "If the test statistics of the true null hypotheses are independent, for the procedure\n",
    "just given,\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q| P_{m_0 + 1} = p_1 , \\ldots , P_m = p_{m_1} ) \\le \\frac{m_0}{m} \\alpha .\n",
    "\\end{equation}\n",
    "\n",
    "**Proof of Lemma.**  \n",
    "Benjamini and Hochberg prove the lemma by induction; the proof is similar to the proof of Simes' inequality.\n",
    "For simplicity, we assume that the test statistics have continuous distributions so that the $P$-values of\n",
    "the true nulls have uniform distributions, but the result is true without that assumption.\n",
    "Suppose $m=1$.\n",
    "Then the procedure rejects $H_1$ if $P_1 \\le \\alpha$.\n",
    "If $m_0 = 0$, no incorrect rejection can occur, so\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q| P_1 = p_1) = 0 \\le \\frac{0}{1} \\alpha .\n",
    "\\end{equation}\n",
    "If $m_0 = 1$, an incorrect rejection occurs if $P_1 \\le \\alpha$.\n",
    "There is no $P_{m_0 + 1}$, so\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q| P_{m_0 + 1} = p_1 , \\ldots , P_m = p_{m_1} )  = \\mathbb{E}(Q) = \\mathbb{P} \\{ P_1 \\le \\alpha \\}\n",
    "= \\alpha \\le \\frac{1}{1} \\alpha .\n",
    "\\end{equation}\n",
    "\n",
    "Suppose that the lemma is true for all $m' \\le m$; we shall show that it is\n",
    "then true for $m' = m+1$.\n",
    "If $m_0 = 0$, the null hypotheses are all false, so $Q$ is identically zero, and\n",
    "the conditional expectation of $Q$, and so\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q| P_1 = p_1, \\ldots, P_m = p_m ) = 0 \\le \\frac{m_0}{m+1} \\alpha .\n",
    "\\end{equation}\n",
    "Suppose $m_0 > 0$. \n",
    "Let $P'_i$, $i = 1, \\ldots, m_0$ be the $P$-values corresponding\n",
    "to the true null hypotheses. Let $P'_{(m_0)}$ be the largest $P'_i$. \n",
    "Since $\\{ P'_i \\}_{i=1}^{m_0}$ are IID $U[0, 1]$, the density of $P'_{(m_0)}$ is $f(u) = m_0 u^{m_0 - 1}$.\n",
    "Let $\\{p_j\\}_{j=1}^{m_1}$ be the $P$-values of the false null hypotheses, ordered\n",
    "so that $p_1 \\le p_2 \\le \\cdots \\le p_{m_1}$.\n",
    "Define \n",
    "\\begin{equation}\n",
    "j_0 :=  \\max \\left \\{j : p_j \\le \\frac{m_0+j}{m+1} \\alpha \\right \\},\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "p_0 = \\frac{m_0 + j_0}{m+1}\\alpha .\n",
    "\\end{equation}\n",
    "Note that $p_{j_0} \\le p_0$.\n",
    "Calculate the expectation, conditioning on the value of $P'_{(m_0)}$:\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}(Q| P_{m_0+1} = p_1, \\ldots, P_m = p_{m_1} ) &=&\n",
    "\\int_0^{p_0} \\mathbb{E} \\left (Q| P'_{(m_0)} = u, P_{m_0+1} = p_1, \\ldots, P_m = p_{m_1} \\right ) f(u) du +\n",
    "\\nonumber \\\\\n",
    "&+& \\int_{p_0}^1 \\mathbb{E}(Q| P'_{(m_0)} = u, P_{m_0+1} = p_1, \\ldots, P_m = p_{m_1} ) f(u) du \n",
    "\\end{eqnarray}\n",
    "In the first integral, $u \\le p_0$, so all the null hypotheses are rejected, and \n",
    "$Q = \\frac{m_0}{m_0 + j_0}$.\n",
    "Recall that $p_0 = \\frac{m_0 + j_0}{m+1} \\alpha$.\n",
    "Thus\n",
    "\\begin{eqnarray}\n",
    "\\int_0^{p_0} \\mathbb{E}(Q| P'_{(m_0)} = u, P_{m_0+1} = p_1, \\ldots, P_m = p_{m_1} ) f(u) du\n",
    "&=&\n",
    "\\int_0^{p_0} \n",
    "\\mathbb{E}(\\frac{m_0}{m_0 + j_0} | P'_{(m_0)} = u, P_{m_0+1} = p_1, \\ldots, P_m = p_{m_1} ) \n",
    "f(u) du\n",
    "\\nonumber \\\\\n",
    "&=&\n",
    "\\int_0^{p_0} \\frac{m_0}{m_0 + j_0} m_0 u^{m_0 - 1} du\n",
    "\\nonumber \\\\\n",
    "&=& \\frac{m_0}{m_0 + j_0} p_0^{m_0}\n",
    "\\nonumber \\\\\n",
    "&= & \\frac{m_0}{m_0 + j_0} \\frac{m_0 + j_0}{m+1} \\alpha p_0^{m_0-1}\n",
    "\\nonumber \\\\\n",
    "&=& \\frac{m_0}{m+1} \\alpha p_0^{m_0 - 1}.\n",
    "\\end{eqnarray}\n",
    "Consider the second integral. \n",
    "On the domain of the second integral, $P'_{(m_0)} = u \\ge p_0 \\ge p_{j_0}$.\n",
    "Here, the true null hypothesis with the largest $P$-value, and possibly other true nulls, will not be rejected.\n",
    "Suppose $j > j_0$ so that $p_{j+1} \\ge p_j \\ge p_{j_0}$.\n",
    "Recall that $p_{j_0} \\le p_0$.\n",
    "Break the domain of integration into the intervals\n",
    "$p_j \\le u \\le p_{j+1}$, $j = j_0 + 1, \\ldots, m_1 - 1$, together with\n",
    "$p_0 \\le u \\le p_{j_0+1}$ and $p_{m_1} \\le u \\le 1$.\n",
    "Because $u, p_{j+1}, \\ldots, p_{m_1}$ are all greater than the threshold\n",
    "value $p_0$, their values\n",
    "cannot result in any hypothesis being rejected.\n",
    "\n",
    "Let $\\{ H_{(i)} \\}_{i=1}^m$ denote the entire set of $m$ null\n",
    "hypotheses, ordered by their $P$-values.\n",
    "In the second integral,\n",
    "the $P$-values of the true null hypotheses are all no larger than $u$\n",
    "(by definition of $u$---it's the largest $P$-value among the true null hypotheses).\n",
    "Recall that the rejection procedure is to reject all hypotheses with smaller $P$-values\n",
    "than $p_0$, so the rejection of $H_{(i)}$ implies that there must be some $k$,\n",
    "$i \\le k \\le m_0 + j - 1$ for which\n",
    "\\begin{equation}\n",
    "p_{(k)} \\le \\frac{k}{m+1} \\alpha.\n",
    "\\end{equation}\n",
    "This is equivalent to\n",
    "\\begin{equation}\n",
    "\\frac{p_{(k)}}{u} \\le \\frac{k}{m_0 + j - 1} \\frac{m_0 + j - 1}{(m+1) u} \\alpha .\n",
    "\\end{equation}\n",
    "The proof is now similar to that of Simes' inequality: conditional on $P'_{(m_0)} = u$,\n",
    "$\\{P'_i/u \\}_{i < m_0}$ are iid $U(0,1)$ random variables; $\\{p_i/u \\}_{i=1}^j$ are\n",
    "some numbers between 0 and 1 corresponding to false null hypotheses.\n",
    "We are testing $m_0 + j - 1 = m' < m$ hypotheses using a different value of $\\alpha$,\n",
    "namely $\\frac{m_0 + j - 1}{(m+1)p} \\alpha$.\n",
    "Because $m' \\le m$, we can apply the induction hypothesis:\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q|P'_{(m_0)} = u, P_{m_0 + 1} = p_1, \\ldots, P_m = p_{m_1} )\n",
    "\\le \\frac{m_0 - 1}{(m+1) u} \\alpha.\n",
    "\\end{equation}\n",
    "This bound does not depend on $p_j$ or $p_{j+1}$,\n",
    "so\n",
    "\\begin{eqnarray}\n",
    "\\int_{p_0}^1 \\mathbb{E}(Q|P'_{(m_0)} = u, P_{m_0 + 1} = p_1, \\ldots, P_m = p_{m_1} )\n",
    "f_{P_{(m_0)}}(u) du &\\le&\n",
    "\\int_{p_0}^1 \\frac{m_0 - 1}{(m+1) u} \\alpha m_0 u^{m_0 - 1} du \n",
    "\\nonumber \\\\\n",
    "&=& \\frac{m_0}{m+1} \\alpha \\int_{p_0}^1 (m_0 - 1) u^{m_0 - 2} du\n",
    "\\nonumber \\\\\n",
    "&=& \\frac{m_0}{m+1} \\alpha ( 1 - p_0^{m_0 - 1}).\n",
    "\\end{eqnarray}\n",
    "Adding this to the bound on the first integral proves the Lemma.\n",
    "\n",
    "\n",
    "**Proof of FDR Theorem.**  \n",
    "Whatever be the joint distribution of $P_{m_0+1}, \\cdots, P_m$ corresponding to the\n",
    "false null hypotheses, integrating the inequality in the Lemma gives\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(Q) = \\mathbb{E}(\\mathbb{E}(Q| P_{m_0+1}, \\cdots, P_m)) \\le \\frac{m_0}{m} \\alpha \\le \\alpha .\n",
    "\\end{equation}\n",
    "\n",
    "The FDR-controlling procedure\n",
    "is equivalent to picking $\\alpha$ _a posteriori_ to maximize the number \n",
    "$r(\\alpha)$ of rejections at that level, subject to the constraint\n",
    "\\begin{equation}\n",
    "\\alpha m / r(\\alpha) \\le \\alpha .\n",
    "\\end{equation}\n",
    "That is, we reject as many hypotheses as possible, subject to the constraint that\n",
    "the expected number of incorrect rejections is at most the \n",
    "FDR times the number of hypotheses actually rejected.\n",
    "The expected number of incorrect rejections is $\\mathbb{E}(V) \\le \\alpha m$, so\n",
    "$Q_e = \\mathbb{E}Q \\le \\alpha m / r(\\alpha) \\le \\alpha$.\n",
    "\n",
    "One complaint about this FDR-controlling procedure (see Shaffer, 1995) is that because $Q$ is defined to be zero when no\n",
    "rejection occurs, the conditional FDR given that some rejection does occur exceeds $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7abfc-5e87-46be-8f0a-277c120c31e7",
   "metadata": {},
   "source": [
    "## Using $E$-values to control the FDR\n",
    "\n",
    "Wang & Ramdas (2020) show that the Benjamini-Hochberg procedure applied to $E$-values instead of $P$-values controls the FDR _under arbitrary dependence of the test statistics_, with no need for the Benjamini-Yekutieli adjustment. \n",
    "\n",
    "In particular, suppose we have an $E$-value $E_j$ for each hypothesis $H_j$, $j \\in \\{1, \\ldots, m\\}$.\n",
    "For $k \\in \\{1, \\ldots, m\\}$, let $E_{[k]}$ be the $k$th order statistic of the $E$-values, ordered from largest to smallest, so that $E_{[1]} \\ge E_{[2]} \\ge \\cdots \\ge E_{[m]}$.\n",
    "The procedure is:\n",
    "\n",
    "+ Let $K_E(\\alpha) := \\max \\left \\{ k \\in \\{1, \\ldots, m\\}: k E_{[k]}/m \\ge 1/\\alpha \\right \\}$\n",
    "+ Reject the hypotheses with the $K_E(\\alpha)$ largest $E$-values.\n",
    "\n",
    "To prove the result requires a bit of new terminology.\n",
    "A testing procedure based on $E$-values is _self-consistent for level $\\alpha$_ if every $E$-value $E_k$ corresponding\n",
    "to a rejected null satisfies\n",
    "\\begin{equation}\n",
    "   E_k \\ge \\frac{m}{\\alpha R},\n",
    "\\end{equation}\n",
    "where $R$ is the number of rejected nulls.\n",
    "This condition is adapted from a similar condition for FDR-controlling procedures based on $P$-values given by\n",
    "Blanchard & Roquain (2008).\n",
    "The idea is that the threshold $E$-value for rejecting a hypothesis should decrease as the FDR (and hence the number of rejections) is allowed to grow. The specific functional form (depending inversely on $R$ and $\\alpha$) is an additional restriction.\n",
    "\n",
    "**Theorem.**  Wang and Ramdas, Proposition 2.  \n",
    "Every $E$-value based test that is self-consistent for level $\\alpha$ has FDR at most $\\alpha m_0/m$.\n",
    "\n",
    "**Proof.**  \n",
    "Let $\\mathbf{E}$ denote the vector of $E$-values; let $\\mathcal{N}$ denote the indices of the true null \n",
    "hypotheses; and let $\\mathcal{R}(\\mathbf{E})$ denote the indices of the null hypotheses that are rejected by the test.\n",
    "The self-consistency condition implies $R \\ge \\frac{m}{\\alpha E_k}$, so\n",
    "\\begin{eqnarray}\n",
    "    Q := \\frac{V}{R \\vee 1} &=& \\frac{|\\mathcal{R}(\\mathbf{E}) \\cap \\mathcal{N}|}{R \\vee 1} \\\\\n",
    "    &=& \\sum_{k \\in \\mathcal{N}} \\frac{1_{k \\in \\mathcal{R}(\\mathbf{E})}}{R \\vee 1} \\\\\n",
    "    &\\le& \\sum_{k \\in \\mathcal{N}} \\frac{1_{k \\in \\mathcal{R}(\\mathbf{E})} \\alpha E_k}{m} \\\\\n",
    "    &\\le& \\sum_{k \\in \\mathcal{N}} \\frac{ \\alpha E_k}{m}.\n",
    "\\end{eqnarray}\n",
    "For the true nulls $k \\in \\mathcal{N}$, $\\mathbb{E}E_k \\le 1$, so\n",
    "\\begin{equation}\n",
    "   \\mathbb{E} Q \\le \\sum_{k \\in \\mathcal{N}} \\mathbb{E} \\frac{\\alpha E_k}{m} \\le \\frac{\\alpha m_0}{m}.\n",
    "\\end{equation}\n",
    "\n",
    "Wang and Ramdas develop the approach much further, including finding situations where the raw $E$-values can be\n",
    "\"boosted,\" and modifications to enforce structure among the rejected nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d74352-ce32-464e-9ef3-42fff87b391c",
   "metadata": {},
   "source": [
    "## Other quantities\n",
    "\n",
    "Genovese and Wasserman (2004) propose controlling a percentile of the false discovery proportion $Q$, rather than the false discovery rate, $\\mathbb{E}Q$. That is, they give a method for which $\\mathbb{P}\\{Q \\ge \\alpha \\} \\le 1-\\gamma$.\n",
    "Similar approaches have been taken by others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e0bd6-b179-4fe0-8a35-93e3c3b168a8",
   "metadata": {},
   "source": [
    "## Analogues for confidence sets\n",
    "\n",
    "### Individual confidence sets\n",
    "\n",
    "### Simultaneous confidence sets\n",
    "\n",
    "### The False Coverage Rate (FCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09611fe-cd87-4baa-a3d6-3c6e3a0afae0",
   "metadata": {},
   "source": [
    "## Selective inference\n",
    "\n",
    "### Error rates for selective confidence sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
